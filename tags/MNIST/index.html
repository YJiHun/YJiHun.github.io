<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Tag: MNIST - 지훈 블로그</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="지훈 블로그"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="지훈 블로그"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="지훈 블로그"><meta property="og:url" content="https://yjihun.github.io/"><meta property="og:site_name" content="지훈 블로그"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://yjihun.github.io/img/og_image.png"><meta property="article:author" content="John Doe"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://YJiHun.github.io"},"headline":"지훈 블로그","image":["https://yjihun.github.io/img/og_image.png"],"author":{"@type":"Person","name":"John Doe"},"publisher":{"@type":"Organization","name":"지훈 블로그","logo":{"@type":"ImageObject","url":null}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">지훈 블로그</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/YJiHun"><i class="fab fa-github"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">MNIST</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-10-21T07:03:00.000Z" title="2021. 10. 21. 오후 4:03:00">2021-10-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-10-22T02:27:20.000Z" title="2021. 10. 22. 오전 11:27:20">2021-10-22</time></span><span class="level-item"><a class="link-muted" href="/categories/MLP/">MLP</a></span><span class="level-item">11 minutes read (About 1666 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2021/10/21/MLP/MLP_1/">MLP 모델을 활용하여 MNIST 모델 개발</a></h1><div class="content"><ul>
<li>개발환경<ul>
<li>Window 10</li>
<li>Python</li>
</ul>
</li>
<li>사용 라이브러리<ul>
<li>numpy</li>
<li>matplotlib</li>
<li>torch</li>
</ul>
</li>
<li>사용 데이터<ul>
<li>MNIST</li>
</ul>
</li>
<li>사용 개발 모델<ul>
<li>MLP(Multi Layer Perceptron)</li>
</ul>
</li>
</ul>
<h2 id="MLP-모델이란"><a href="#MLP-모델이란" class="headerlink" title="MLP 모델이란?"></a>MLP 모델이란?</h2><ul>
<li>퍼셉트론이 지니고 있는 한계(비선형 분류 문제 해결)를 극복하기 위해 여러 Layer를 쌓아올린 MLP(Multi Layer Perceptron)가 등장함. </li>
<li>간단히 비교하면, <ul>
<li>퍼셉트론은 Input Layer와 Output Layer만 존재하는 형태입니다. </li>
<li>MLP는 Input과 Output 사이에 Hidden Layer를 추가합니다. 즉, MLP는 이러한 Hidden Layer를 여러겹으로 쌓게 되는데, 이를 MLP 모델이라고 부릅니다. </li>
</ul>
</li>
</ul>
<h2 id="MLP-모델을-활용한-MNIST-모델-개발"><a href="#MLP-모델을-활용한-MNIST-모델-개발" class="headerlink" title="MLP 모델을 활용한 MNIST 모델 개발"></a>MLP 모델을 활용한 MNIST 모델 개발</h2><ul>
<li>MLP 모델 순서대로 코드를 구현합니다. </li>
</ul>
<h3 id="Step-1-모듈-불러오기"><a href="#Step-1-모듈-불러오기" class="headerlink" title="Step 1. 모듈 불러오기"></a>Step 1. 모듈 불러오기</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, datasets</span><br></pre></td></tr></table></figure>

<h3 id="Step-2-딥러닝-모델-설계-시-필요한-장비-세팅"><a href="#Step-2-딥러닝-모델-설계-시-필요한-장비-세팅" class="headerlink" title="Step 2. 딥러닝 모델 설계 시 필요한 장비 세팅"></a>Step 2. 딥러닝 모델 설계 시 필요한 장비 세팅</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">  DEVICE = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  DEVICE = torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;PyTorch Version:&quot;</span>, torch.__version__, <span class="string">&#x27; Device:&#x27;</span>, DEVICE)</span><br></pre></td></tr></table></figure>

<pre><code>PyTorch Version: 1.9.0+cu111  Device: cuda
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">32</span> <span class="comment"># 데이터가 32개로 구성되어 있음. </span></span><br><span class="line">EPOCHS = <span class="number">10</span> <span class="comment"># 전체 데이터 셋을 10번 반복해 학습함.</span></span><br></pre></td></tr></table></figure>

<h3 id="Step-3-데이터-다운로드"><a href="#Step-3-데이터-다운로드" class="headerlink" title="Step 3. 데이터 다운로드"></a>Step 3. 데이터 다운로드</h3><ul>
<li>torchvision 내 datasets 함수 이용하여 데이터셋 다운로드 합니다.</li>
<li>ToTensor() 활용하여 데이터셋을 tensor 형태로 변환</li>
<li>한 픽셀은 0<del>255 범위의 스칼라 값으로 구성, 이를 0</del>1 범위에서 정규화 과정 진행</li>
<li>DataLoader는 일종의 Batch Size 만큼 묶음으로 묶어준다는 의미<ul>
<li>Batch_size는 Mini-batch 1개 단위를 구성하는 데이터의 개수</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = datasets.MNIST(root = <span class="string">&quot;../data/MNIST&quot;</span>, </span><br><span class="line">                               train = <span class="literal">True</span>, </span><br><span class="line">                               download = <span class="literal">True</span>, </span><br><span class="line">                               transform = transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">test_dataset = datasets.MNIST(root = <span class="string">&quot;../data/MNIST&quot;</span>, </span><br><span class="line">                              train = <span class="literal">False</span>, </span><br><span class="line">                              transform = transforms.ToTensor())</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset = train_dataset, </span><br><span class="line">                                           batch_size = BATCH_SIZE, </span><br><span class="line">                                           shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">test_loader = torch.utils.data.DataLoader(dataset = test_dataset, </span><br><span class="line">                                           batch_size = BATCH_SIZE, </span><br><span class="line">                                           shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h3 id="step-4-데이터-확인-및-시각화"><a href="#step-4-데이터-확인-및-시각화" class="headerlink" title="step 4. 데이터 확인 및 시각화"></a>step 4. 데이터 확인 및 시각화</h3><ul>
<li>데이터를 확인하고 시각화를 진행합니다. </li>
<li>32개의 이미지 데이터에 label 값이 각 1개씩 존재하기 때문에 32개의 값을 갖고 있음</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (X_train, y_train) <span class="keyword">in</span> train_loader:</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&#x27;X_train:&#x27;</span>, X_train.size(), <span class="string">&#x27;type:&#x27;</span>, X_train.<span class="built_in">type</span>())</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&#x27;y_train:&#x27;</span>, y_train.size(), <span class="string">&#x27;type:&#x27;</span>, y_train.<span class="built_in">type</span>())</span><br><span class="line">  <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<pre><code>X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor
y_train: torch.Size([32]) type: torch.LongTensor
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pltsize = <span class="number">1</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span> * pltsize, pltsize))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">  plt.subplot(<span class="number">1</span>, <span class="number">10</span>, i + <span class="number">1</span>)</span><br><span class="line">  plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">  plt.imshow(X_train[i, :, :, :].numpy().reshape(<span class="number">28</span>, <span class="number">28</span>), cmap=<span class="string">&quot;gray_r&quot;</span>)</span><br><span class="line">  plt.title(<span class="string">&#x27;Class: &#x27;</span> + <span class="built_in">str</span>(y_train[i].item()))</span><br></pre></td></tr></table></figure>


<p><img src="/images/MLP/output_12_0.png"></p>
<h3 id="step-5-MLP-모델-설계"><a href="#step-5-MLP-모델-설계" class="headerlink" title="step 5. MLP 모델 설계"></a>step 5. MLP 모델 설계</h3><ul>
<li>torch 모듈을 이용해 MLP를 설계합니다. </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">  <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  Forward Propagation 정의</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">    self.fc1 = nn.Linear(<span class="number">28</span> * <span class="number">28</span> * <span class="number">1</span>, <span class="number">512</span>) <span class="comment"># (가로 픽셀 * 세로 픽셀 * 채널 수) 크기의 노드 수 설정 Fully Connected Layer 노드 수 512개 설정</span></span><br><span class="line">    self.fc2 = nn.Linear(<span class="number">512</span>, <span class="number">256</span>) <span class="comment"># Input으로 사용할 노드 수는 512으로, Output 노드수는 256개로 지정</span></span><br><span class="line">    self.fc3 = nn.Linear(<span class="number">256</span>, <span class="number">10</span>) <span class="comment"># Input 노드수는 256, Output 노드수는 10개로 지정</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    x = x.view(-<span class="number">1</span>, <span class="number">28</span> * <span class="number">28</span>) <span class="comment"># 1차원으로 펼친 이미지 데이터 통과</span></span><br><span class="line">    x = self.fc1(x)</span><br><span class="line">    x = F.sigmoid(x)</span><br><span class="line">    x = self.fc2(x)</span><br><span class="line">    x = F.sigmoid(x)</span><br><span class="line">    x = self.fc3(x)</span><br><span class="line">    x = F.log_softmax(x, dim = <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h3 id="step-6-옵티마이저-목적-함수-설정"><a href="#step-6-옵티마이저-목적-함수-설정" class="headerlink" title="step 6. 옵티마이저 목적 함수 설정"></a>step 6. 옵티마이저 목적 함수 설정</h3><ul>
<li>Back Propagation 설정 위한 목적 함수 설정 </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = Net().to(DEVICE)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr = <span class="number">0.01</span>, momentum=<span class="number">0.5</span>)</span><br><span class="line">criterion = nn.CrossEntropyLoss() <span class="comment"># output 값과 원-핫 인코딩 값과의 Loss </span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>

<pre><code>Net(
  (fc1): Linear(in_features=784, out_features=512, bias=True)
  (fc2): Linear(in_features=512, out_features=256, bias=True)
  (fc3): Linear(in_features=256, out_features=10, bias=True)
)
</code></pre>
<h3 id="step-7-MLP-모델-학습"><a href="#step-7-MLP-모델-학습" class="headerlink" title="step 7. MLP 모델 학습"></a>step 7. MLP 모델 학습</h3><ul>
<li>MLP 모델을 학습 상태로 지정하는 코드를 구현 </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">model, train_loader, optimizer, log_interval</span>):</span></span><br><span class="line">  model.train()</span><br><span class="line">  <span class="keyword">for</span> batch_idx, (image, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader): <span class="comment"># 모형 학습</span></span><br><span class="line">    image = image.to(DEVICE)</span><br><span class="line">    label = label.to(DEVICE)</span><br><span class="line">    optimizer.zero_grad() <span class="comment"># Optimizer의 Gradient 초기화</span></span><br><span class="line">    output = model(image)</span><br><span class="line">    loss = criterion(output, label)</span><br><span class="line">    loss.backward() <span class="comment"># back propagation 계산</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> batch_idx % log_interval == <span class="number">0</span>:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&quot;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125;(&#123;:.0f&#125;%)]\tTrain Loass: &#123;:.6f&#125;&quot;</span>.<span class="built_in">format</span>(Epoch, batch_idx * <span class="built_in">len</span>(image), </span><br><span class="line">                                                                           <span class="built_in">len</span>(train_loader.dataset), <span class="number">100.</span> * batch_idx / <span class="built_in">len</span>(train_loader), loss.item()))</span><br></pre></td></tr></table></figure>

<h3 id="step-8-검증-데이터-확인-함수"><a href="#step-8-검증-데이터-확인-함수" class="headerlink" title="step 8. 검증 데이터 확인 함수"></a>step 8. 검증 데이터 확인 함수</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span>(<span class="params">model, test_loader</span>):</span></span><br><span class="line">  model.<span class="built_in">eval</span>()</span><br><span class="line">  test_loss = <span class="number">0</span></span><br><span class="line">  correct = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> image, label <span class="keyword">in</span> test_loader:</span><br><span class="line">      image = image.to(DEVICE)</span><br><span class="line">      label = label.to(DEVICE)</span><br><span class="line">      output = model(image)</span><br><span class="line">      test_loss += criterion(output, label).item()</span><br><span class="line">      prediction = output.<span class="built_in">max</span>(<span class="number">1</span>, keepdim = <span class="literal">True</span>)[<span class="number">1</span>]</span><br><span class="line">      correct += prediction.eq(label.view_as(prediction)).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">  test_loss /= <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">  test_accuracy = <span class="number">100.</span> * correct / <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">  <span class="keyword">return</span> test_loss, test_accuracy</span><br></pre></td></tr></table></figure>

<ul>
<li>모델 평가 시, Gradient를 통해 파라미터 값이 업데이트되는 현상 방지 위해 torch.no_grad() Gradient의 흐름 제어</li>
</ul>
<h3 id="step-9-MLP-학습-실행"><a href="#step-9-MLP-학습-실행" class="headerlink" title="step 9. MLP 학습 실행"></a>step 9. MLP 학습 실행</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> Epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, EPOCHS + <span class="number">1</span>):</span><br><span class="line">  train(model, train_loader, optimizer, log_interval=<span class="number">200</span>)</span><br><span class="line">  test_loss, test_accuracy = evaluate(model, test_loader)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;\n[EPOCH: &#123;&#125;], \tTest Loss: &#123;:.4f&#125;, \tTest Accuracy: &#123;:.2f&#125; %\n&quot;</span>.<span class="built_in">format</span>(Epoch, test_loss, test_accuracy))</span><br></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn(&quot;nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.&quot;)


Train Epoch: 1 [0/60000(0%)]    Train Loass: 2.271369
Train Epoch: 1 [6400/60000(11%)]    Train Loass: 2.338554
Train Epoch: 1 [12800/60000(21%)]    Train Loass: 2.303339
Train Epoch: 1 [19200/60000(32%)]    Train Loass: 2.286205
Train Epoch: 1 [25600/60000(43%)]    Train Loass: 2.301424
Train Epoch: 1 [32000/60000(53%)]    Train Loass: 2.316196
Train Epoch: 1 [38400/60000(64%)]    Train Loass: 2.281273
Train Epoch: 1 [44800/60000(75%)]    Train Loass: 2.274917
Train Epoch: 1 [51200/60000(85%)]    Train Loass: 2.223652
Train Epoch: 1 [57600/60000(96%)]    Train Loass: 2.307122

[EPOCH: 1],     Test Loss: 0.0703,     Test Accuracy: 27.32 %

Train Epoch: 2 [0/60000(0%)]    Train Loass: 2.231371
Train Epoch: 2 [6400/60000(11%)]    Train Loass: 2.193915
Train Epoch: 2 [12800/60000(21%)]    Train Loass: 2.192727
Train Epoch: 2 [19200/60000(32%)]    Train Loass: 2.111516
Train Epoch: 2 [25600/60000(43%)]    Train Loass: 1.988003
Train Epoch: 2 [32000/60000(53%)]    Train Loass: 1.900740
Train Epoch: 2 [38400/60000(64%)]    Train Loass: 1.676852
Train Epoch: 2 [44800/60000(75%)]    Train Loass: 1.568495
Train Epoch: 2 [51200/60000(85%)]    Train Loass: 1.486937
Train Epoch: 2 [57600/60000(96%)]    Train Loass: 1.287047

[EPOCH: 2],     Test Loss: 0.0397,     Test Accuracy: 63.09 %

Train Epoch: 3 [0/60000(0%)]    Train Loass: 1.430903
Train Epoch: 3 [6400/60000(11%)]    Train Loass: 1.077034
Train Epoch: 3 [12800/60000(21%)]    Train Loass: 1.064919
Train Epoch: 3 [19200/60000(32%)]    Train Loass: 0.905477
Train Epoch: 3 [25600/60000(43%)]    Train Loass: 0.938080
Train Epoch: 3 [32000/60000(53%)]    Train Loass: 0.872440
Train Epoch: 3 [38400/60000(64%)]    Train Loass: 0.839272
Train Epoch: 3 [44800/60000(75%)]    Train Loass: 0.676609
Train Epoch: 3 [51200/60000(85%)]    Train Loass: 0.697175
Train Epoch: 3 [57600/60000(96%)]    Train Loass: 0.629446

[EPOCH: 3],     Test Loss: 0.0239,     Test Accuracy: 76.73 %

Train Epoch: 4 [0/60000(0%)]    Train Loass: 0.853515
Train Epoch: 4 [6400/60000(11%)]    Train Loass: 0.748754
Train Epoch: 4 [12800/60000(21%)]    Train Loass: 0.794022
Train Epoch: 4 [19200/60000(32%)]    Train Loass: 0.503913
Train Epoch: 4 [25600/60000(43%)]    Train Loass: 0.641133
Train Epoch: 4 [32000/60000(53%)]    Train Loass: 0.721150
Train Epoch: 4 [38400/60000(64%)]    Train Loass: 0.780429
Train Epoch: 4 [44800/60000(75%)]    Train Loass: 0.578635
Train Epoch: 4 [51200/60000(85%)]    Train Loass: 0.703849
Train Epoch: 4 [57600/60000(96%)]    Train Loass: 0.439490

[EPOCH: 4],     Test Loss: 0.0175,     Test Accuracy: 83.77 %

Train Epoch: 5 [0/60000(0%)]    Train Loass: 0.805851
Train Epoch: 5 [6400/60000(11%)]    Train Loass: 0.530511
Train Epoch: 5 [12800/60000(21%)]    Train Loass: 0.668718
Train Epoch: 5 [19200/60000(32%)]    Train Loass: 0.360271
Train Epoch: 5 [25600/60000(43%)]    Train Loass: 0.665644
Train Epoch: 5 [32000/60000(53%)]    Train Loass: 0.437206
Train Epoch: 5 [38400/60000(64%)]    Train Loass: 0.590770
Train Epoch: 5 [44800/60000(75%)]    Train Loass: 0.462136
Train Epoch: 5 [51200/60000(85%)]    Train Loass: 0.324244
Train Epoch: 5 [57600/60000(96%)]    Train Loass: 0.340563

[EPOCH: 5],     Test Loss: 0.0143,     Test Accuracy: 86.79 %

Train Epoch: 6 [0/60000(0%)]    Train Loass: 0.449360
Train Epoch: 6 [6400/60000(11%)]    Train Loass: 0.412948
Train Epoch: 6 [12800/60000(21%)]    Train Loass: 0.861725
Train Epoch: 6 [19200/60000(32%)]    Train Loass: 0.261632
Train Epoch: 6 [25600/60000(43%)]    Train Loass: 0.420589
Train Epoch: 6 [32000/60000(53%)]    Train Loass: 0.343440
Train Epoch: 6 [38400/60000(64%)]    Train Loass: 0.583203
Train Epoch: 6 [44800/60000(75%)]    Train Loass: 0.677286
Train Epoch: 6 [51200/60000(85%)]    Train Loass: 0.410981
Train Epoch: 6 [57600/60000(96%)]    Train Loass: 0.520130

[EPOCH: 6],     Test Loss: 0.0128,     Test Accuracy: 88.20 %

Train Epoch: 7 [0/60000(0%)]    Train Loass: 0.437152
Train Epoch: 7 [6400/60000(11%)]    Train Loass: 0.477764
Train Epoch: 7 [12800/60000(21%)]    Train Loass: 0.372617
Train Epoch: 7 [19200/60000(32%)]    Train Loass: 0.300505
Train Epoch: 7 [25600/60000(43%)]    Train Loass: 0.347319
Train Epoch: 7 [32000/60000(53%)]    Train Loass: 0.584562
Train Epoch: 7 [38400/60000(64%)]    Train Loass: 0.352436
Train Epoch: 7 [44800/60000(75%)]    Train Loass: 0.484784
Train Epoch: 7 [51200/60000(85%)]    Train Loass: 0.552677
Train Epoch: 7 [57600/60000(96%)]    Train Loass: 0.385262

[EPOCH: 7],     Test Loss: 0.0118,     Test Accuracy: 89.14 %

Train Epoch: 8 [0/60000(0%)]    Train Loass: 0.217616
Train Epoch: 8 [6400/60000(11%)]    Train Loass: 0.445853
Train Epoch: 8 [12800/60000(21%)]    Train Loass: 0.299457
Train Epoch: 8 [19200/60000(32%)]    Train Loass: 0.296233
Train Epoch: 8 [25600/60000(43%)]    Train Loass: 0.375333
Train Epoch: 8 [32000/60000(53%)]    Train Loass: 0.142711
Train Epoch: 8 [38400/60000(64%)]    Train Loass: 0.271160
Train Epoch: 8 [44800/60000(75%)]    Train Loass: 0.379447
Train Epoch: 8 [51200/60000(85%)]    Train Loass: 0.343264
Train Epoch: 8 [57600/60000(96%)]    Train Loass: 0.303766

[EPOCH: 8],     Test Loss: 0.0114,     Test Accuracy: 89.65 %

Train Epoch: 9 [0/60000(0%)]    Train Loass: 0.320234
Train Epoch: 9 [6400/60000(11%)]    Train Loass: 0.502022
Train Epoch: 9 [12800/60000(21%)]    Train Loass: 0.255956
Train Epoch: 9 [19200/60000(32%)]    Train Loass: 0.592139
Train Epoch: 9 [25600/60000(43%)]    Train Loass: 0.299304
Train Epoch: 9 [32000/60000(53%)]    Train Loass: 0.337499
Train Epoch: 9 [38400/60000(64%)]    Train Loass: 0.361548
Train Epoch: 9 [44800/60000(75%)]    Train Loass: 0.323064
Train Epoch: 9 [51200/60000(85%)]    Train Loass: 0.211570
Train Epoch: 9 [57600/60000(96%)]    Train Loass: 0.259323

[EPOCH: 9],     Test Loss: 0.0109,     Test Accuracy: 89.95 %

Train Epoch: 10 [0/60000(0%)]    Train Loass: 0.467115
Train Epoch: 10 [6400/60000(11%)]    Train Loass: 0.494733
Train Epoch: 10 [12800/60000(21%)]    Train Loass: 0.191910
Train Epoch: 10 [19200/60000(32%)]    Train Loass: 0.404155
Train Epoch: 10 [25600/60000(43%)]    Train Loass: 0.402310
Train Epoch: 10 [32000/60000(53%)]    Train Loass: 0.228435
Train Epoch: 10 [38400/60000(64%)]    Train Loass: 0.308584
Train Epoch: 10 [44800/60000(75%)]    Train Loass: 0.702611
Train Epoch: 10 [51200/60000(85%)]    Train Loass: 0.492218
Train Epoch: 10 [57600/60000(96%)]    Train Loass: 0.420322

[EPOCH: 10],     Test Loss: 0.0105,     Test Accuracy: 90.36 %
</code></pre>
<ul>
<li>train 함수 실행하면, model은 기존에 정의한 MLP 모델, train_loader는 학습 데이터, optimizer는 SGD, log_interval은 학습이 진행되면서 mini-batch index를 이용해 과정을 모니터링할 수 있도록 출력함.</li>
<li>학습 완료 시, Test Accuracy는 90% 수준의 정확도를 나타냄. </li>
</ul>
<p>-</p>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="You Ji Hun"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">You Ji Hun</p><p class="is-size-6 is-block">Student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Daegu</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">3</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">2</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">3</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/YJiHun"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://github.com/YJiHun/YJH" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">YJH</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li><li><a class="level is-mobile" href="https://github.com/YJiHun/YJH_Project" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">YJH_Project</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/GitHub/"><span class="level-start"><span class="level-item">GitHub</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/MLP/"><span class="level-start"><span class="level-item">MLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-10-23T05:52:53.000Z">2021-10-23</time></p><p class="title"><a href="/2021/10/23/new-post/">new post</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-10-22T00:19:27.000Z">2021-10-22</time></p><p class="title"><a href="/2021/10/22/GitHub/GitHub-setting-1/">GitHub를 사용하기위한 기본 세팅</a></p><p class="categories"><a href="/categories/GitHub/">GitHub</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2021-10-21T07:03:00.000Z">2021-10-21</time></p><p class="title"><a href="/2021/10/21/MLP/MLP_1/">MLP 모델을 활용하여 MNIST 모델 개발</a></p><p class="categories"><a href="/categories/MLP/">MLP</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">October 2021</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/GitHub/"><span class="tag">GitHub</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MLP/"><span class="tag">MLP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MNIST/"><span class="tag">MNIST</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">지훈 블로그</a><p class="is-size-7"><span>&copy; 2021 John Doe</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/YJiHun"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><!--!--><script src="/js/main.js" defer></script><!--!--></body></html>