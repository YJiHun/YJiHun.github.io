{"pages":[],"posts":[{"title":"GitHub를 사용하기위한 기본 세팅_1","text":"사용환경 Window 10 Git 홈페이지 Download 클릭 사용중인 OS 클릭 다운로드 경로 이동하여 실행파일(.exe) 실행 Git 2.33.1 Setup 실행후 Next 클릭 Install 클릭 Finish 클릭 Git 설치완료temp..","link":"/2021/10/22/GitHub/GitHub-setting-1/"},{"title":"MLP 모델을 활용하여 MNIST 모델 개발","text":"개발환경 Window 10 Python 사용 라이브러리 numpy matplotlib torch 사용 데이터 MNIST 사용 개발 모델 MLP(Multi Layer Perceptron) MLP 모델이란? 퍼셉트론이 지니고 있는 한계(비선형 분류 문제 해결)를 극복하기 위해 여러 Layer를 쌓아올린 MLP(Multi Layer Perceptron)가 등장함. 간단히 비교하면, 퍼셉트론은 Input Layer와 Output Layer만 존재하는 형태입니다. MLP는 Input과 Output 사이에 Hidden Layer를 추가합니다. 즉, MLP는 이러한 Hidden Layer를 여러겹으로 쌓게 되는데, 이를 MLP 모델이라고 부릅니다. MLP 모델을 활용한 MNIST 모델 개발 MLP 모델 순서대로 코드를 구현합니다. Step 1. 모듈 불러오기123456import numpy as npimport matplotlib.pyplot as pltimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torchvision import transforms, datasets Step 2. 딥러닝 모델 설계 시 필요한 장비 세팅123456if torch.cuda.is_available(): DEVICE = torch.device('cuda')else: DEVICE = torch.device('cpu')print(&quot;PyTorch Version:&quot;, torch.__version__, ' Device:', DEVICE) PyTorch Version: 1.9.0+cu111 Device: cuda 12BATCH_SIZE = 32 # 데이터가 32개로 구성되어 있음. EPOCHS = 10 # 전체 데이터 셋을 10번 반복해 학습함. Step 3. 데이터 다운로드 torchvision 내 datasets 함수 이용하여 데이터셋 다운로드 합니다. ToTensor() 활용하여 데이터셋을 tensor 형태로 변환 한 픽셀은 0255 범위의 스칼라 값으로 구성, 이를 01 범위에서 정규화 과정 진행 DataLoader는 일종의 Batch Size 만큼 묶음으로 묶어준다는 의미 Batch_size는 Mini-batch 1개 단위를 구성하는 데이터의 개수 12345678910111213141516train_dataset = datasets.MNIST(root = &quot;../data/MNIST&quot;, train = True, download = True, transform = transforms.ToTensor())test_dataset = datasets.MNIST(root = &quot;../data/MNIST&quot;, train = False, transform = transforms.ToTensor())train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle=True)test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle=False) step 4. 데이터 확인 및 시각화 데이터를 확인하고 시각화를 진행합니다. 32개의 이미지 데이터에 label 값이 각 1개씩 존재하기 때문에 32개의 값을 갖고 있음 1234for (X_train, y_train) in train_loader: print('X_train:', X_train.size(), 'type:', X_train.type()) print('y_train:', y_train.size(), 'type:', y_train.type()) break X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor y_train: torch.Size([32]) type: torch.LongTensor 1234567pltsize = 1plt.figure(figsize=(10 * pltsize, pltsize))for i in range(10): plt.subplot(1, 10, i + 1) plt.axis('off') plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap=&quot;gray_r&quot;) plt.title('Class: ' + str(y_train[i].item())) step 5. MLP 모델 설계 torch 모듈을 이용해 MLP를 설계합니다. 12345678910111213141516171819class Net(nn.Module): ''' Forward Propagation 정의 ''' def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(28 * 28 * 1, 512) # (가로 픽셀 * 세로 픽셀 * 채널 수) 크기의 노드 수 설정 Fully Connected Layer 노드 수 512개 설정 self.fc2 = nn.Linear(512, 256) # Input으로 사용할 노드 수는 512으로, Output 노드수는 256개로 지정 self.fc3 = nn.Linear(256, 10) # Input 노드수는 256, Output 노드수는 10개로 지정 def forward(self, x): x = x.view(-1, 28 * 28) # 1차원으로 펼친 이미지 데이터 통과 x = self.fc1(x) x = F.sigmoid(x) x = self.fc2(x) x = F.sigmoid(x) x = self.fc3(x) x = F.log_softmax(x, dim = 1) return x step 6. 옵티마이저 목적 함수 설정 Back Propagation 설정 위한 목적 함수 설정 1234model = Net().to(DEVICE)optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.5)criterion = nn.CrossEntropyLoss() # output 값과 원-핫 인코딩 값과의 Loss print(model) Net( (fc1): Linear(in_features=784, out_features=512, bias=True) (fc2): Linear(in_features=512, out_features=256, bias=True) (fc3): Linear(in_features=256, out_features=10, bias=True) ) step 7. MLP 모델 학습 MLP 모델을 학습 상태로 지정하는 코드를 구현 1234567891011121314def train(model, train_loader, optimizer, log_interval): model.train() for batch_idx, (image, label) in enumerate(train_loader): # 모형 학습 image = image.to(DEVICE) label = label.to(DEVICE) optimizer.zero_grad() # Optimizer의 Gradient 초기화 output = model(image) loss = criterion(output, label) loss.backward() # back propagation 계산 optimizer.step() if batch_idx % log_interval == 0: print(&quot;Train Epoch: {} [{}/{}({:.0f}%)]\\tTrain Loass: {:.6f}&quot;.format(Epoch, batch_idx * len(image), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) step 8. 검증 데이터 확인 함수1234567891011121314151617def evaluate(model, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for image, label in test_loader: image = image.to(DEVICE) label = label.to(DEVICE) output = model(image) test_loss += criterion(output, label).item() prediction = output.max(1, keepdim = True)[1] correct += prediction.eq(label.view_as(prediction)).sum().item() test_loss /= len(test_loader.dataset) test_accuracy = 100. * correct / len(test_loader.dataset) return test_loss, test_accuracy 모델 평가 시, Gradient를 통해 파라미터 값이 업데이트되는 현상 방지 위해 torch.no_grad() Gradient의 흐름 제어 step 9. MLP 학습 실행1234for Epoch in range(1, EPOCHS + 1): train(model, train_loader, optimizer, log_interval=200) test_loss, test_accuracy = evaluate(model, test_loader) print(&quot;\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n&quot;.format(Epoch, test_loss, test_accuracy)) /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead. warnings.warn(&quot;nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.&quot;) Train Epoch: 1 [0/60000(0%)] Train Loass: 2.271369 Train Epoch: 1 [6400/60000(11%)] Train Loass: 2.338554 Train Epoch: 1 [12800/60000(21%)] Train Loass: 2.303339 Train Epoch: 1 [19200/60000(32%)] Train Loass: 2.286205 Train Epoch: 1 [25600/60000(43%)] Train Loass: 2.301424 Train Epoch: 1 [32000/60000(53%)] Train Loass: 2.316196 Train Epoch: 1 [38400/60000(64%)] Train Loass: 2.281273 Train Epoch: 1 [44800/60000(75%)] Train Loass: 2.274917 Train Epoch: 1 [51200/60000(85%)] Train Loass: 2.223652 Train Epoch: 1 [57600/60000(96%)] Train Loass: 2.307122 [EPOCH: 1], Test Loss: 0.0703, Test Accuracy: 27.32 % Train Epoch: 2 [0/60000(0%)] Train Loass: 2.231371 Train Epoch: 2 [6400/60000(11%)] Train Loass: 2.193915 Train Epoch: 2 [12800/60000(21%)] Train Loass: 2.192727 Train Epoch: 2 [19200/60000(32%)] Train Loass: 2.111516 Train Epoch: 2 [25600/60000(43%)] Train Loass: 1.988003 Train Epoch: 2 [32000/60000(53%)] Train Loass: 1.900740 Train Epoch: 2 [38400/60000(64%)] Train Loass: 1.676852 Train Epoch: 2 [44800/60000(75%)] Train Loass: 1.568495 Train Epoch: 2 [51200/60000(85%)] Train Loass: 1.486937 Train Epoch: 2 [57600/60000(96%)] Train Loass: 1.287047 [EPOCH: 2], Test Loss: 0.0397, Test Accuracy: 63.09 % Train Epoch: 3 [0/60000(0%)] Train Loass: 1.430903 Train Epoch: 3 [6400/60000(11%)] Train Loass: 1.077034 Train Epoch: 3 [12800/60000(21%)] Train Loass: 1.064919 Train Epoch: 3 [19200/60000(32%)] Train Loass: 0.905477 Train Epoch: 3 [25600/60000(43%)] Train Loass: 0.938080 Train Epoch: 3 [32000/60000(53%)] Train Loass: 0.872440 Train Epoch: 3 [38400/60000(64%)] Train Loass: 0.839272 Train Epoch: 3 [44800/60000(75%)] Train Loass: 0.676609 Train Epoch: 3 [51200/60000(85%)] Train Loass: 0.697175 Train Epoch: 3 [57600/60000(96%)] Train Loass: 0.629446 [EPOCH: 3], Test Loss: 0.0239, Test Accuracy: 76.73 % Train Epoch: 4 [0/60000(0%)] Train Loass: 0.853515 Train Epoch: 4 [6400/60000(11%)] Train Loass: 0.748754 Train Epoch: 4 [12800/60000(21%)] Train Loass: 0.794022 Train Epoch: 4 [19200/60000(32%)] Train Loass: 0.503913 Train Epoch: 4 [25600/60000(43%)] Train Loass: 0.641133 Train Epoch: 4 [32000/60000(53%)] Train Loass: 0.721150 Train Epoch: 4 [38400/60000(64%)] Train Loass: 0.780429 Train Epoch: 4 [44800/60000(75%)] Train Loass: 0.578635 Train Epoch: 4 [51200/60000(85%)] Train Loass: 0.703849 Train Epoch: 4 [57600/60000(96%)] Train Loass: 0.439490 [EPOCH: 4], Test Loss: 0.0175, Test Accuracy: 83.77 % Train Epoch: 5 [0/60000(0%)] Train Loass: 0.805851 Train Epoch: 5 [6400/60000(11%)] Train Loass: 0.530511 Train Epoch: 5 [12800/60000(21%)] Train Loass: 0.668718 Train Epoch: 5 [19200/60000(32%)] Train Loass: 0.360271 Train Epoch: 5 [25600/60000(43%)] Train Loass: 0.665644 Train Epoch: 5 [32000/60000(53%)] Train Loass: 0.437206 Train Epoch: 5 [38400/60000(64%)] Train Loass: 0.590770 Train Epoch: 5 [44800/60000(75%)] Train Loass: 0.462136 Train Epoch: 5 [51200/60000(85%)] Train Loass: 0.324244 Train Epoch: 5 [57600/60000(96%)] Train Loass: 0.340563 [EPOCH: 5], Test Loss: 0.0143, Test Accuracy: 86.79 % Train Epoch: 6 [0/60000(0%)] Train Loass: 0.449360 Train Epoch: 6 [6400/60000(11%)] Train Loass: 0.412948 Train Epoch: 6 [12800/60000(21%)] Train Loass: 0.861725 Train Epoch: 6 [19200/60000(32%)] Train Loass: 0.261632 Train Epoch: 6 [25600/60000(43%)] Train Loass: 0.420589 Train Epoch: 6 [32000/60000(53%)] Train Loass: 0.343440 Train Epoch: 6 [38400/60000(64%)] Train Loass: 0.583203 Train Epoch: 6 [44800/60000(75%)] Train Loass: 0.677286 Train Epoch: 6 [51200/60000(85%)] Train Loass: 0.410981 Train Epoch: 6 [57600/60000(96%)] Train Loass: 0.520130 [EPOCH: 6], Test Loss: 0.0128, Test Accuracy: 88.20 % Train Epoch: 7 [0/60000(0%)] Train Loass: 0.437152 Train Epoch: 7 [6400/60000(11%)] Train Loass: 0.477764 Train Epoch: 7 [12800/60000(21%)] Train Loass: 0.372617 Train Epoch: 7 [19200/60000(32%)] Train Loass: 0.300505 Train Epoch: 7 [25600/60000(43%)] Train Loass: 0.347319 Train Epoch: 7 [32000/60000(53%)] Train Loass: 0.584562 Train Epoch: 7 [38400/60000(64%)] Train Loass: 0.352436 Train Epoch: 7 [44800/60000(75%)] Train Loass: 0.484784 Train Epoch: 7 [51200/60000(85%)] Train Loass: 0.552677 Train Epoch: 7 [57600/60000(96%)] Train Loass: 0.385262 [EPOCH: 7], Test Loss: 0.0118, Test Accuracy: 89.14 % Train Epoch: 8 [0/60000(0%)] Train Loass: 0.217616 Train Epoch: 8 [6400/60000(11%)] Train Loass: 0.445853 Train Epoch: 8 [12800/60000(21%)] Train Loass: 0.299457 Train Epoch: 8 [19200/60000(32%)] Train Loass: 0.296233 Train Epoch: 8 [25600/60000(43%)] Train Loass: 0.375333 Train Epoch: 8 [32000/60000(53%)] Train Loass: 0.142711 Train Epoch: 8 [38400/60000(64%)] Train Loass: 0.271160 Train Epoch: 8 [44800/60000(75%)] Train Loass: 0.379447 Train Epoch: 8 [51200/60000(85%)] Train Loass: 0.343264 Train Epoch: 8 [57600/60000(96%)] Train Loass: 0.303766 [EPOCH: 8], Test Loss: 0.0114, Test Accuracy: 89.65 % Train Epoch: 9 [0/60000(0%)] Train Loass: 0.320234 Train Epoch: 9 [6400/60000(11%)] Train Loass: 0.502022 Train Epoch: 9 [12800/60000(21%)] Train Loass: 0.255956 Train Epoch: 9 [19200/60000(32%)] Train Loass: 0.592139 Train Epoch: 9 [25600/60000(43%)] Train Loass: 0.299304 Train Epoch: 9 [32000/60000(53%)] Train Loass: 0.337499 Train Epoch: 9 [38400/60000(64%)] Train Loass: 0.361548 Train Epoch: 9 [44800/60000(75%)] Train Loass: 0.323064 Train Epoch: 9 [51200/60000(85%)] Train Loass: 0.211570 Train Epoch: 9 [57600/60000(96%)] Train Loass: 0.259323 [EPOCH: 9], Test Loss: 0.0109, Test Accuracy: 89.95 % Train Epoch: 10 [0/60000(0%)] Train Loass: 0.467115 Train Epoch: 10 [6400/60000(11%)] Train Loass: 0.494733 Train Epoch: 10 [12800/60000(21%)] Train Loass: 0.191910 Train Epoch: 10 [19200/60000(32%)] Train Loass: 0.404155 Train Epoch: 10 [25600/60000(43%)] Train Loass: 0.402310 Train Epoch: 10 [32000/60000(53%)] Train Loass: 0.228435 Train Epoch: 10 [38400/60000(64%)] Train Loass: 0.308584 Train Epoch: 10 [44800/60000(75%)] Train Loass: 0.702611 Train Epoch: 10 [51200/60000(85%)] Train Loass: 0.492218 Train Epoch: 10 [57600/60000(96%)] Train Loass: 0.420322 [EPOCH: 10], Test Loss: 0.0105, Test Accuracy: 90.36 % train 함수 실행하면, model은 기존에 정의한 MLP 모델, train_loader는 학습 데이터, optimizer는 SGD, log_interval은 학습이 진행되면서 mini-batch index를 이용해 과정을 모니터링할 수 있도록 출력함. 학습 완료 시, Test Accuracy는 90% 수준의 정확도를 나타냄. -","link":"/2021/10/21/MLP/MLP_1/"},{"title":"GitBlog를 시작하자","text":"사용환경 Window 10 필수 파일 설치 1단계 nodejs.org 다운로드 홈페이지 접속 다운로드 다운 실행파일(.exe) 실행 Next 클릭 체크박스 체크후 =&gt; Next 클릭 Next 클릭 체크박스 체크후 =&gt; Next 클릭 Install 클릭 cmd창이 자동으로 꺼질때까지 기다립니다. cmd 창에서 node.js 다운로드 확인을 합니다. node -v ## v14.18.1 2단계 Git 다운로드 Git 다운로드가 되지않은경우 아래 링크를 들어가셔서 다운로드 진행 Git 다운로드 cmd 창에서 Git 다운로드 확인을 합니다. git --version ## git version 2.33.1.windows.1 3단계 Hexo 설치 cmd 창 npm install -g hexo-cli GitHub 설정 GitHub에 두개의 Repositories를 생성합니다. 포스트 버전관리 Repositories 포스트 배포용 관리 Repositories(Your_Username.github.io) GitBash창 포스트 버전관리 파일을 내려받을 위치에 마우스 오른쪽 클릭 =&gt; Git Bash Here 클릭 git clone Your_git_repositories_address.git GitBlog 만들기 블로그 파일을 넣을 임의의 폴더생성 ex) blogcmd 창 hexo init blog ## cmd 창에서 생성한 임의의 폴더 바로 상위 폴더로 이동 cd blog npm install npm install hexo-server --save npm install hexo-deployer-git --save _config.yml파일 설정 페이지 정보수정 title: 제목을 지어주세요 subtitle: 부제목을 지어주세요 description: description을 지어주세요 author: YourName 블로그 URL 정보 설정 url: https://Your_Username.github.io root: / permalink: :year/:month/:day/:title/ permalink_defaults: GitHub 연동 deploy: type: git repo: https://github.com/Your_Username/Your_Username.github.io.git branch: main GitHub 배포하기 배포전 미리보기 hexo generate ## 파일을 넣은경우 hexo server 배포하기 hexo generate hexo deploy GitHub 포스트 추가 포스트추가 hexo new post “타이틀 명” scaffolds ㄴpost.md 초기 양식 변경법 # post.mdd --- title: {{ title }} date: {{ date }} tags: categories: --- GitBlog End","link":"/2021/10/22/GitHub/GitBlog/GitBlog-Start/"},{"title":"Threading을 해보자","text":"CPU : 개발환경 Window10 Python CPU : i7-6700 사용 라이브러리 threading Threading이란?ㅇㄹㄴㅁㅇ 1234567891011121314import threadingfrom queue import Queueimport time as t``` 쓰래딩을 하기위해선 함수를 생성하여 쓰래드를 실행 시켜야합니다.- 함수 생성```pythondef count(num): result = 0 for i in range(num): result += 1 pass return result pass 1234567def Count(num, q): result = 0 for i in range(num): result += 1 pass q.put(result) pass 실행단1234567891011121314151617181920212223242526272829303132if __name__ == '__main__': qu = Queue() Num = 100000000 number = 0 start_time = t.time() ## 그냥 카운트 a = count(Num) print(a) print(&quot;Normal&quot;, t.time() - start_time, &quot;sec&quot;) start_time = t.time() ## 쓰레드 카운트 A = threading.Thread(target=Count, args=(Num // 4, qu)) B = threading.Thread(target=Count, args=(Num // 4, qu)) C = threading.Thread(target=Count, args=(Num // 4, qu)) D = threading.Thread(target=Count, args=(Num // 4, qu)) A.start() B.start() C.start() D.start() while True: if not qu.empty(): number += qu.get() if number == Num: print(number) break print(&quot;Threading&quot;, t.time() - start_time, &quot;sec&quot;) pass 100000000 Normal 6.2418341636657715 sec 100000000 Threading 8.561893939971924 sec 쓰래딩은","link":"/2021/10/23/Thread/Thread/"},{"title":"Fashion_Minist_dataset을 이용하여 딥러닝 공부","text":"데이터 1. 데이터 불러오기123from tensorflow.keras import datasetsimport matplotlib.pyplot as pltimport numpy as np 123(x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()x_train.shape, y_train.shape, x_test.shape, x_test.shape ((60000, 28, 28), (60000,), (10000, 28, 28), (10000, 28, 28)) 2. 데이터 시각화 (EDA)12345678910plt.figure(figsize=(8, 8))for i in range(30): plt.subplot(5, 6, i + 1) img = x_train[i] label = y_train[i] plt.imshow(img, cmap= 'gray') plt.title(label) plt.xticks([]) plt.yticks([])plt.show() 이미지 확인 1print(x_train[0]) [[ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 13 73 0 0 1 4 0 0 0 0 1 1 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 3 0 36 136 127 62 54 0 0 0 1 3 4 0 0 3] [ 0 0 0 0 0 0 0 0 0 0 0 0 6 0 102 204 176 134 144 123 23 0 0 0 0 12 10 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 155 236 207 178 107 156 161 109 64 23 77 130 72 15] [ 0 0 0 0 0 0 0 0 0 0 0 1 0 69 207 223 218 216 216 163 127 121 122 146 141 88 172 66] [ 0 0 0 0 0 0 0 0 0 1 1 1 0 200 232 232 233 229 223 223 215 213 164 127 123 196 229 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 183 225 216 223 228 235 227 224 222 224 221 223 245 173 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 193 228 218 213 198 180 212 210 211 213 223 220 243 202 0] [ 0 0 0 0 0 0 0 0 0 1 3 0 12 219 220 212 218 192 169 227 208 218 224 212 226 197 209 52] [ 0 0 0 0 0 0 0 0 0 0 6 0 99 244 222 220 218 203 198 221 215 213 222 220 245 119 167 56] [ 0 0 0 0 0 0 0 0 0 4 0 0 55 236 228 230 228 240 232 213 218 223 234 217 217 209 92 0] [ 0 0 1 4 6 7 2 0 0 0 0 0 237 226 217 223 222 219 222 221 216 223 229 215 218 255 77 0] [ 0 3 0 0 0 0 0 0 0 62 145 204 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159 0] [ 0 0 0 0 18 44 82 107 189 228 220 222 217 226 200 205 211 230 224 234 176 188 250 248 233 238 215 0] [ 0 57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246 0] [ 3 202 228 224 221 211 211 214 205 205 205 220 240 80 150 255 229 221 188 154 191 210 204 209 222 228 225 0] [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241 65 73 106 117 168 219 221 215 217 223 223 224 229 29] [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245 239 223 218 212 209 222 220 221 230 67] [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172 181 205 206 115] [ 0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191 195 191 198 192 176 156 167 177 210 92] [ 0 0 74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188 188 194 192 216 170 0] [ 2 0 0 0 66 200 222 237 239 242 246 243 244 221 220 193 191 179 182 182 181 176 166 168 99 58 0 0] [ 0 0 0 0 0 0 0 40 61 44 72 41 35 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] 실제 데이터 확인 데이터 시각화 1234plt.title('Data distribution')plt.hist(np.reshape(x_train, (60000*28*28)),log=True, bins=50, alpha=0.8)# plt.hist(np.reshape(x_train, (1, -1)))plt.show() 3. 데이터 전처리 정규화 원핫벡터 min max normalization$\\frac{x-x_{min}}{x_{max}-x_{min}}$123456def minmax(x): x_min = np.min(x) x_max = np.max(x) # print(x_min, x_max) result = (x - x_min) / (x_max - x_min) return result 1234x_train_minmax = minmax(x_train)x_test_minmax = minmax(x_test)x_train_minmax.shape, x_test_minmax.shape ((60000, 28, 28), (10000, 28, 28)) 12345678910plt.figure(figsize=(10, 4))plt.subplot(1, 2, 1)plt.title('Train data minmax normed distribution')plt.hist(np.reshape(x_train_minmax, (60000*28*28)),log=True, bins=50, alpha=0.8)plt.subplot(1, 2, 2)plt.title('Test data minmax normed distribution')plt.hist(np.reshape(x_test_minmax, (10000*28*28)),log=True, bins=50, alpha=0.8)plt.show() Z-Score Normalization$\\frac{x-\\bar{x}}{\\sigma}$12345def z_score(x): x_mean = np.mean(x) x_std = np.std(x) result = (x - x_mean) / x_std return result 1234x_train_z_score = z_score(x_train)x_test_z_score = z_score(x_test)x_train_z_score.shape, x_test_z_score.shape ((60000, 28, 28), (10000, 28, 28)) 12345678910plt.figure(figsize=(10, 4))plt.subplot(1, 2, 1)plt.title('Train data minmax normed distribution')plt.hist(np.reshape(x_train_z_score, (60000*28*28)),log=True, bins=50, alpha=0.8)plt.subplot(1, 2, 2)plt.title('Test data minmax normed distribution')plt.hist(np.reshape(x_test_z_score, (10000*28*28)),log=True, bins=50, alpha=0.8)plt.show() one-hot coding1from tensorflow.keras.utils import to_categorical 1234y_train_onehot = to_categorical(y_train, num_classes =10)y_test_onehot = to_categorical(y_test, num_classes =10)y_train_onehot.shape, y_test_onehot.shape ((60000, 10), (10000, 10)) 1y_train_onehot array([[0., 0., 0., ..., 0., 0., 1.], [1., 0., 0., ..., 0., 0., 0.], [1., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [1., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32) 모델 1from tensorflow.keras import models, layers, optimizers 1234sr = models.Sequential(name='Softmax_regression')sr.add(layers.Flatten(input_shape=[28, 28]))sr.add(layers.Dense(10, activation='softmax'))sr.summary() Model: &quot;Softmax_regression&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 _________________________________________________________________ dense (Dense) (None, 10) 7850 ================================================================= Total params: 7,850 Trainable params: 7,850 Non-trainable params: 0 _________________________________________________________________ 1. 모델구현 1 Softmax regression1 DNN123456789DNN = models.Sequential(name='DNN')#Input layerDNN.add(layers.Flatten(input_shape=[28, 28]))#hidden layerDNN.add(layers.Dense(100, activation='relu'))DNN.add(layers.Dense(100, activation='relu'))#output layerDNN.add(layers.Dense(10, activation='softmax'))DNN.summary() Model: &quot;DNN&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_1 (Flatten) (None, 784) 0 _________________________________________________________________ dense_1 (Dense) (None, 100) 78500 _________________________________________________________________ dense_2 (Dense) (None, 100) 10100 _________________________________________________________________ dense_3 (Dense) (None, 10) 1010 ================================================================= Total params: 89,610 Trainable params: 89,610 Non-trainable params: 0 _________________________________________________________________ CNN1234567891011121314151617CNN = models.Sequential(name='CNN')# x data : (28, 28) -&gt; 2차원# dense : (764) -&gt; 1차원# convolution : (28, 28, 1) -&gt; 3차원#Input layerCNN.add(layers.Reshape([28, 28, 1], input_shape=[28, 28]))#hidden layerCNN.add(layers.Conv2D(10, kernel_size = 5, activation='relu'))CNN.add(layers.MaxPool2D(pool_size = 3))CNN.add(layers.Conv2D(10, kernel_size = 5, activation='relu'))CNN.add(layers.MaxPool2D(pool_size = 3))#output layerCNN.add(layers.Flatten())CNN.add(layers.Dense(10, activation='softmax'))CNN.summary() Model: &quot;CNN&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= reshape (Reshape) (None, 28, 28, 1) 0 _________________________________________________________________ conv2d (Conv2D) (None, 24, 24, 10) 260 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 8, 8, 10) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 4, 4, 10) 2510 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 1, 1, 10) 0 _________________________________________________________________ flatten_2 (Flatten) (None, 10) 0 _________________________________________________________________ dense_4 (Dense) (None, 10) 110 ================================================================= Total params: 2,880 Trainable params: 2,880 Non-trainable params: 0 _________________________________________________________________ 2. 학습 Softmax Regression1234sr.compile(optimizer=optimizers.SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['acc'])sr.fit(x_train_minmax, y_train_onehot, epochs=20) Epoch 1/20 1875/1875 [==============================] - 7s 2ms/step - loss: 0.8363 - acc: 0.7333 Epoch 2/20 1875/1875 [==============================] - 5s 2ms/step - loss: 0.5929 - acc: 0.8083 Epoch 3/20 1875/1875 [==============================] - 5s 2ms/step - loss: 0.5426 - acc: 0.8217 Epoch 4/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5157 - acc: 0.8290 Epoch 5/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.4979 - acc: 0.8332 Epoch 6/20 1875/1875 [==============================] - 5s 2ms/step - loss: 0.4851 - acc: 0.8375 Epoch 7/20 1875/1875 [==============================] - 5s 2ms/step - loss: 0.4759 - acc: 0.8407 Epoch 8/20 1875/1875 [==============================] - 5s 2ms/step - loss: 0.4680 - acc: 0.8425 Epoch 9/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.4614 - acc: 0.8445 Epoch 10/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.4560 - acc: 0.8463 Epoch 11/20 1875/1875 [==============================] - 5s 2ms/step - loss: 0.4511 - acc: 0.8471 Epoch 12/20 1875/1875 [==============================] - 5s 2ms/step - loss: 0.4475 - acc: 0.8488 Epoch 13/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.4437 - acc: 0.8498 Epoch 14/20 1875/1875 [==============================] - 5s 2ms/step - loss: 0.4408 - acc: 0.8504 Epoch 15/20 1875/1875 [==============================] - 5s 2ms/step - loss: 0.4375 - acc: 0.8516 Epoch 16/20 1875/1875 [==============================] - 5s 2ms/step - loss: 0.4349 - acc: 0.8525 Epoch 17/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.4323 - acc: 0.8536 Epoch 18/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.4301 - acc: 0.8530 Epoch 19/20 1875/1875 [==============================] - 5s 2ms/step - loss: 0.4285 - acc: 0.8536 Epoch 20/20 1875/1875 [==============================] - 5s 2ms/step - loss: 0.4259 - acc: 0.8547 &lt;keras.callbacks.History at 0x7efffc300710&gt; DNN1234DNN.compile(optimizer=optimizers.SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['acc'])DNN.fit(x_train_minmax, y_train_onehot, epochs=20) Epoch 1/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.7562 - acc: 0.7513 Epoch 2/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.4967 - acc: 0.8285 Epoch 3/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.4516 - acc: 0.8424 Epoch 4/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.4235 - acc: 0.8524 Epoch 5/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.4036 - acc: 0.8583 Epoch 6/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.3882 - acc: 0.8637 Epoch 7/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.3738 - acc: 0.8681 Epoch 8/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.3630 - acc: 0.8720 Epoch 9/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.3536 - acc: 0.8753 Epoch 10/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.3431 - acc: 0.8785 Epoch 11/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.3352 - acc: 0.8812 Epoch 12/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.3277 - acc: 0.8832 Epoch 13/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.3204 - acc: 0.8855 Epoch 14/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.3143 - acc: 0.8866 Epoch 15/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.3086 - acc: 0.8903 Epoch 16/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.3025 - acc: 0.8907 Epoch 17/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.2979 - acc: 0.8932 Epoch 18/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.2927 - acc: 0.8936 Epoch 19/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.2880 - acc: 0.8956 Epoch 20/20 1875/1875 [==============================] - 5s 3ms/step - loss: 0.2838 - acc: 0.8980 &lt;keras.callbacks.History at 0x7efffbe57710&gt; CNN1234CNN.compile(optimizer=optimizers.SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['acc'])CNN.fit(x_train_minmax, y_train_onehot, epochs=20) Epoch 1/20 1875/1875 [==============================] - 34s 4ms/step - loss: 1.1147 - acc: 0.6055 Epoch 2/20 1875/1875 [==============================] - 7s 3ms/step - loss: 0.6989 - acc: 0.7430 Epoch 3/20 1875/1875 [==============================] - 7s 3ms/step - loss: 0.6344 - acc: 0.7631 Epoch 4/20 1875/1875 [==============================] - 6s 3ms/step - loss: 0.5965 - acc: 0.7770 Epoch 5/20 1875/1875 [==============================] - 7s 4ms/step - loss: 0.5700 - acc: 0.7872 Epoch 6/20 1875/1875 [==============================] - 7s 4ms/step - loss: 0.5478 - acc: 0.7989 Epoch 7/20 1875/1875 [==============================] - 7s 4ms/step - loss: 0.5295 - acc: 0.8071 Epoch 8/20 1875/1875 [==============================] - 7s 3ms/step - loss: 0.5152 - acc: 0.8119 Epoch 9/20 1875/1875 [==============================] - 7s 3ms/step - loss: 0.5025 - acc: 0.8163 Epoch 10/20 1875/1875 [==============================] - 7s 4ms/step - loss: 0.4914 - acc: 0.8212 Epoch 11/20 1875/1875 [==============================] - 7s 4ms/step - loss: 0.4820 - acc: 0.8258 Epoch 12/20 1875/1875 [==============================] - 7s 4ms/step - loss: 0.4728 - acc: 0.8275 Epoch 13/20 1875/1875 [==============================] - 7s 4ms/step - loss: 0.4653 - acc: 0.8311 Epoch 14/20 1875/1875 [==============================] - 7s 4ms/step - loss: 0.4561 - acc: 0.8336 Epoch 15/20 1875/1875 [==============================] - 7s 4ms/step - loss: 0.4469 - acc: 0.8376 Epoch 16/20 1875/1875 [==============================] - 7s 4ms/step - loss: 0.4390 - acc: 0.8407 Epoch 17/20 1875/1875 [==============================] - 7s 4ms/step - loss: 0.4311 - acc: 0.8440 Epoch 18/20 1875/1875 [==============================] - 7s 4ms/step - loss: 0.4239 - acc: 0.8462 Epoch 19/20 1875/1875 [==============================] - 7s 4ms/step - loss: 0.4185 - acc: 0.8486 Epoch 20/20 1875/1875 [==============================] - 7s 4ms/step - loss: 0.4112 - acc: 0.8516 &lt;keras.callbacks.History at 0x7efffa642850&gt; 3. 성능평가 Softmax Regression12345# evaluate : 성능 평가 test를 하고싶을때 (labels이 있는경우)# predict : 값 예측을 하고싶을때 (labels이 없는경우)sr_score = sr.evaluate(x_test_minmax, y_test_onehot, verbose = 1)print(&quot;accuracy :&quot;, sr_score[1], &quot;loss :&quot;, sr_score[0]) 313/313 [==============================] - 1s 3ms/step - loss: 0.4655 - acc: 0.8365 accuracy : 0.8364999890327454 loss : 0.46547895669937134 DNN123DNN_score = DNN.evaluate(x_test_minmax, y_test_onehot, verbose = 1)print(&quot;accuracy :&quot;, DNN_score[1], &quot;loss :&quot;, DNN_score[0]) 313/313 [==============================] - 1s 3ms/step - loss: 0.3544 - acc: 0.8713 accuracy : 0.8712999820709229 loss : 0.3544173538684845 CNN123CNN_score = CNN.evaluate(x_test_minmax, y_test_onehot, verbose = 1)print(&quot;accuracy :&quot;, CNN_score[1], &quot;loss :&quot;, CNN_score[0]) 313/313 [==============================] - 1s 3ms/step - loss: 0.4489 - acc: 0.8395 accuracy : 0.8395000100135803 loss : 0.4489458501338959","link":"/2021/10/27/DeepLearning/Fashion_Mnist_dataset/6_Fashion_Mnist_dataset/"},{"title":"Cifar10_dataset을 이용하여 딥러닝 공부","text":"데이터 1. 데이터 불러오기123from tensorflow.keras import datasetsimport matplotlib.pyplot as pltimport numpy as np 123(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()x_train.shape, y_train.shape, x_test.shape, x_test.shape ((50000, 32, 32, 3), (50000, 1), (10000, 32, 32, 3), (10000, 32, 32, 3)) 2. 데이터 시각화 (EDA)12345678910plt.figure(figsize=(8, 8))for i in range(30): plt.subplot(5, 6, i + 1) img = x_train[i] label = y_train[i] plt.imshow(img, cmap= 'gray') plt.title(label) plt.xticks([]) plt.yticks([])plt.show() /usr/local/lib/python3.7/dist-packages/matplotlib/text.py:1165: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison if s != self._text: 이미지 확인 1print(x_train[0]) [[[ 59 62 63] [ 43 46 45] [ 50 48 43] ... [158 132 108] [152 125 102] [148 124 103]] [[ 16 20 20] [ 0 0 0] [ 18 8 0] ... [123 88 55] [119 83 50] [122 87 57]] [[ 25 24 21] [ 16 7 0] [ 49 27 8] ... [118 84 50] [120 84 50] [109 73 42]] ... [[208 170 96] [201 153 34] [198 161 26] ... [160 133 70] [ 56 31 7] [ 53 34 20]] [[180 139 96] [173 123 42] [186 144 30] ... [184 148 94] [ 97 62 34] [ 83 53 34]] [[177 144 116] [168 129 94] [179 142 87] ... [216 184 140] [151 118 84] [123 92 72]]] 실제 데이터 확인 데이터 시각화 1234plt.title('Data distribution')plt.hist(np.reshape(x_train, (50000*32*32*3)),log=True, bins=50, alpha=0.8)# plt.hist(np.reshape(x_train, (1, -1)))plt.show() 3. 데이터 전처리 정규화 원핫벡터 min max normalization$\\frac{x-x_{min}}{x_{max}-x_{min}}$123456def minmax(x): x_min = np.min(x) x_max = np.max(x) # print(x_min, x_max) result = (x - x_min) / (x_max - x_min) return result 1234x_train_minmax = minmax(x_train)x_test_minmax = minmax(x_test)x_train_minmax.shape, x_test_minmax.shape ((50000, 32, 32, 3), (10000, 32, 32, 3)) 12345678910plt.figure(figsize=(10, 4))plt.subplot(1, 2, 1)plt.title('Train data minmax normed distribution')plt.hist(np.reshape(x_train_minmax, (50000*32*32*3)),log=True, bins=50, alpha=0.8)plt.subplot(1, 2, 2)plt.title('Test data minmax normed distribution')plt.hist(np.reshape(x_test_minmax, (10000*32*32*3)),log=True, bins=50, alpha=0.8)plt.show() Z-Score Normalization$\\frac{x-\\bar{x}}{\\sigma}$12345def z_score(x): x_mean = np.mean(x) x_std = np.std(x) result = (x - x_mean) / x_std return result 1234x_train_z_score = z_score(x_train)x_test_z_score = z_score(x_test)x_train_z_score.shape, x_test_z_score.shape ((50000, 32, 32, 3), (10000, 32, 32, 3)) 12345678910plt.figure(figsize=(10, 4))plt.subplot(1, 2, 1)plt.title('Train data minmax normed distribution')plt.hist(np.reshape(x_train_z_score, (50000*32*32*3)),log=True, bins=50, alpha=0.8)plt.subplot(1, 2, 2)plt.title('Test data minmax normed distribution')plt.hist(np.reshape(x_test_z_score, (10000*32*32*3)),log=True, bins=50, alpha=0.8)plt.show() one-hot coding1from tensorflow.keras.utils import to_categorical 1234y_train_onehot = to_categorical(y_train, num_classes =10)y_test_onehot = to_categorical(y_test, num_classes =10)y_train_onehot.shape, y_test_onehot.shape ((50000, 10), (10000, 10)) 1y_train_onehot array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 1.], [0., 0., 0., ..., 0., 0., 1.], ..., [0., 0., 0., ..., 0., 0., 1.], [0., 1., 0., ..., 0., 0., 0.], [0., 1., 0., ..., 0., 0., 0.]], dtype=float32) 모델 1from tensorflow.keras import models, layers, optimizers 1234sr = models.Sequential(name='Softmax_regression')sr.add(layers.Flatten(input_shape=[32, 32, 3]))sr.add(layers.Dense(10, activation='softmax'))sr.summary() Model: &quot;Softmax_regression&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_19 (Flatten) (None, 3072) 0 _________________________________________________________________ dense_18 (Dense) (None, 10) 30730 ================================================================= Total params: 30,730 Trainable params: 30,730 Non-trainable params: 0 _________________________________________________________________ 1. 모델구현 1 Softmax regression1 DNN123456789DNN = models.Sequential(name='DNN')#Input layerDNN.add(layers.Flatten(input_shape=[32, 32, 3]))#hidden layerDNN.add(layers.Dense(100, activation='relu'))DNN.add(layers.Dense(100, activation='relu'))#output layerDNN.add(layers.Dense(10, activation='softmax'))DNN.summary() Model: &quot;DNN&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_20 (Flatten) (None, 3072) 0 _________________________________________________________________ dense_19 (Dense) (None, 100) 307300 _________________________________________________________________ dense_20 (Dense) (None, 100) 10100 _________________________________________________________________ dense_21 (Dense) (None, 10) 1010 ================================================================= Total params: 318,410 Trainable params: 318,410 Non-trainable params: 0 _________________________________________________________________ CNN123456789101112131415161718CNN = models.Sequential(name='CNN')# x data : (28, 28) -&gt; 2차원# dense : (764) -&gt; 1차원# convolution : (28, 28, 1) -&gt; 3차원#Input layerCNN.add(layers.Reshape([32, 32, 3], input_shape=[32, 32, 3]))# CNN.add(layers.Flatten(input_shape = [32, 32, 3]))#hidden layerCNN.add(layers.Conv2D(10, kernel_size = 5, activation='relu'))CNN.add(layers.MaxPool2D(pool_size = 3))CNN.add(layers.Conv2D(10, kernel_size = 5, activation='relu'))CNN.add(layers.MaxPool2D(pool_size = 3))#output layerCNN.add(layers.Flatten())CNN.add(layers.Dense(10, activation='softmax'))CNN.summary() Model: &quot;CNN&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= reshape_6 (Reshape) (None, 32, 32, 3) 0 _________________________________________________________________ conv2d_11 (Conv2D) (None, 28, 28, 10) 760 _________________________________________________________________ max_pooling2d_6 (MaxPooling2 (None, 9, 9, 10) 0 _________________________________________________________________ conv2d_12 (Conv2D) (None, 5, 5, 10) 2510 _________________________________________________________________ max_pooling2d_7 (MaxPooling2 (None, 1, 1, 10) 0 _________________________________________________________________ flatten_21 (Flatten) (None, 10) 0 _________________________________________________________________ dense_22 (Dense) (None, 10) 110 ================================================================= Total params: 3,380 Trainable params: 3,380 Non-trainable params: 0 _________________________________________________________________ 2. 학습 Softmax Regression1234sr.compile(optimizer=optimizers.SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['acc'])sr.fit(x_train_minmax, y_train_onehot, epochs=20) Epoch 1/20 1563/1563 [==============================] - 8s 3ms/step - loss: 1.9475 - acc: 0.3043 Epoch 2/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.8404 - acc: 0.3543 Epoch 3/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.8150 - acc: 0.3650 Epoch 4/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.7946 - acc: 0.3737 Epoch 5/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.7831 - acc: 0.3779 Epoch 6/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.7728 - acc: 0.3838 Epoch 7/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.7674 - acc: 0.3821 Epoch 8/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.7582 - acc: 0.3895 Epoch 9/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.7522 - acc: 0.3927 Epoch 10/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.7467 - acc: 0.3933 Epoch 11/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.7426 - acc: 0.3962 Epoch 12/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.7393 - acc: 0.3966 Epoch 13/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.7319 - acc: 0.4015 Epoch 14/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.7296 - acc: 0.4004 Epoch 15/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.7303 - acc: 0.3997 Epoch 16/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.7267 - acc: 0.4037 Epoch 17/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.7242 - acc: 0.4018 Epoch 18/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.7214 - acc: 0.4049 Epoch 19/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.7194 - acc: 0.4059 Epoch 20/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.7161 - acc: 0.4086 &lt;keras.callbacks.History at 0x7fa6a575cb10&gt; DNN1234DNN.compile(optimizer=optimizers.SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['acc'])DNN.fit(x_train_minmax, y_train_onehot, epochs=20) Epoch 1/20 1563/1563 [==============================] - 6s 3ms/step - loss: 1.9164 - acc: 0.3105 Epoch 2/20 1563/1563 [==============================] - 5s 4ms/step - loss: 1.7298 - acc: 0.3856 Epoch 3/20 1563/1563 [==============================] - 6s 4ms/step - loss: 1.6466 - acc: 0.4165 Epoch 4/20 1563/1563 [==============================] - 6s 4ms/step - loss: 1.5964 - acc: 0.4329 Epoch 5/20 1563/1563 [==============================] - 5s 4ms/step - loss: 1.5514 - acc: 0.4513 Epoch 6/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.5182 - acc: 0.4627 Epoch 7/20 1563/1563 [==============================] - 6s 4ms/step - loss: 1.4865 - acc: 0.4740 Epoch 8/20 1563/1563 [==============================] - 6s 4ms/step - loss: 1.4621 - acc: 0.4831 Epoch 9/20 1563/1563 [==============================] - 6s 4ms/step - loss: 1.4402 - acc: 0.4893 Epoch 10/20 1563/1563 [==============================] - 6s 4ms/step - loss: 1.4179 - acc: 0.4969 Epoch 11/20 1563/1563 [==============================] - 6s 4ms/step - loss: 1.3974 - acc: 0.5056 Epoch 12/20 1563/1563 [==============================] - 5s 4ms/step - loss: 1.3817 - acc: 0.5103 Epoch 13/20 1563/1563 [==============================] - 5s 3ms/step - loss: 1.3635 - acc: 0.5169 Epoch 14/20 1563/1563 [==============================] - 6s 4ms/step - loss: 1.3485 - acc: 0.5206 Epoch 15/20 1563/1563 [==============================] - 6s 4ms/step - loss: 1.3333 - acc: 0.5272 Epoch 16/20 1563/1563 [==============================] - 6s 4ms/step - loss: 1.3202 - acc: 0.5307 Epoch 17/20 1563/1563 [==============================] - 6s 4ms/step - loss: 1.3058 - acc: 0.5370 Epoch 18/20 1563/1563 [==============================] - 6s 4ms/step - loss: 1.2915 - acc: 0.5434 Epoch 19/20 1563/1563 [==============================] - 6s 4ms/step - loss: 1.2782 - acc: 0.5468 Epoch 20/20 1563/1563 [==============================] - 6s 4ms/step - loss: 1.2670 - acc: 0.5487 &lt;keras.callbacks.History at 0x7fa6a72738d0&gt; CNN1234CNN.compile(optimizer=optimizers.SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['acc'])CNN.fit(x_train_minmax, y_train_onehot, epochs=20) Epoch 1/20 1563/1563 [==============================] - 34s 5ms/step - loss: 2.2529 - acc: 0.1649 Epoch 2/20 1563/1563 [==============================] - 7s 4ms/step - loss: 2.0787 - acc: 0.2463 Epoch 3/20 1563/1563 [==============================] - 7s 4ms/step - loss: 1.9293 - acc: 0.2835 Epoch 4/20 1563/1563 [==============================] - 7s 5ms/step - loss: 1.8375 - acc: 0.3172 Epoch 5/20 1563/1563 [==============================] - 7s 5ms/step - loss: 1.7666 - acc: 0.3460 Epoch 6/20 1563/1563 [==============================] - 7s 5ms/step - loss: 1.6964 - acc: 0.3763 Epoch 7/20 1563/1563 [==============================] - 7s 5ms/step - loss: 1.6457 - acc: 0.3998 Epoch 8/20 1563/1563 [==============================] - 7s 5ms/step - loss: 1.6103 - acc: 0.4169 Epoch 9/20 1563/1563 [==============================] - 7s 5ms/step - loss: 1.5839 - acc: 0.4256 Epoch 10/20 1563/1563 [==============================] - 7s 4ms/step - loss: 1.5658 - acc: 0.4327 Epoch 11/20 1563/1563 [==============================] - 7s 5ms/step - loss: 1.5445 - acc: 0.4410 Epoch 12/20 1563/1563 [==============================] - 7s 4ms/step - loss: 1.5313 - acc: 0.4492 Epoch 13/20 1563/1563 [==============================] - 7s 4ms/step - loss: 1.5190 - acc: 0.4527 Epoch 14/20 1563/1563 [==============================] - 7s 4ms/step - loss: 1.5042 - acc: 0.4570 Epoch 15/20 1563/1563 [==============================] - 7s 4ms/step - loss: 1.4925 - acc: 0.4628 Epoch 16/20 1563/1563 [==============================] - 7s 4ms/step - loss: 1.4811 - acc: 0.4684 Epoch 17/20 1563/1563 [==============================] - 7s 4ms/step - loss: 1.4682 - acc: 0.4751 Epoch 18/20 1563/1563 [==============================] - 7s 4ms/step - loss: 1.4580 - acc: 0.4788 Epoch 19/20 1563/1563 [==============================] - 7s 4ms/step - loss: 1.4467 - acc: 0.4819 Epoch 20/20 1563/1563 [==============================] - 7s 4ms/step - loss: 1.4370 - acc: 0.4872 &lt;keras.callbacks.History at 0x7fa627b0a450&gt; 3. 성능평가 Softmax Regression12345# evaluate : 성능 평가 test를 하고싶을때 (labels이 있는경우)# predict : 값 예측을 하고싶을때 (labels이 없는경우)sr_score = sr.evaluate(x_test_minmax, y_test_onehot, verbose = 1)print(&quot;accuracy :&quot;, sr_score[1], &quot;loss :&quot;, sr_score[0]) 313/313 [==============================] - 1s 3ms/step - loss: 1.8597 - acc: 0.3399 accuracy : 0.3398999869823456 loss : 1.8596729040145874 DNN123DNN_score = DNN.evaluate(x_test_minmax, y_test_onehot, verbose = 1)print(&quot;accuracy :&quot;, DNN_score[1], &quot;loss :&quot;, DNN_score[0]) 313/313 [==============================] - 1s 3ms/step - loss: 1.4524 - acc: 0.4859 accuracy : 0.48590001463890076 loss : 1.4524061679840088 CNN123CNN_score = CNN.evaluate(x_test_minmax, y_test_onehot, verbose = 1)print(&quot;accuracy :&quot;, CNN_score[1], &quot;loss :&quot;, CNN_score[0]) 313/313 [==============================] - 1s 4ms/step - loss: 1.4791 - acc: 0.4728 accuracy : 0.47279998660087585 loss : 1.4790728092193604","link":"/2021/10/27/DeepLearning/CIFAR_dataset/6_CIFAR_dataset/"},{"title":"Iris_dataset을 이용하여 딥러닝 공부","text":"1. 데이터 정의 sklearn 으로 부터 iris 데이터셋을 불러옵니다. x_data와 y_data를 정의합니다. 1! ls iris_test.csv iris_train.csv sample_data sample_submission.csv 12import pandas as pdimport numpy as np 1test_data = pd.read_csv('iris_test.csv') 12train_data = pd.read_csv('iris_train.csv')train_data .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id species sepal length (cm) petal length (cm) sepal width (cm) petal width (cm) 0 0 setosa 4.4 1.4 2.9 0.2 1 1 versicolor 6.4 4.5 3.2 1.5 2 2 virginica 6.2 4.8 2.8 1.8 3 3 virginica 7.2 6.1 3.6 2.5 4 4 setosa 4.9 1.4 3.0 0.2 ... ... ... ... ... ... ... 70 70 versicolor 6.5 4.6 2.8 1.5 71 71 versicolor 5.6 3.6 2.9 1.3 72 72 versicolor 6.2 4.5 2.2 1.5 73 73 versicolor 4.9 3.3 2.4 1.0 74 74 versicolor 6.9 4.9 3.1 1.5 75 rows × 6 columns 1234567data_np = train_data.to_numpy()x_data = data_np[:, 2:].astype('float')y_data = data_np[:, 1]print(x_data.shape)print(y_data.shape) (75, 4) (75,) 1x_data 1y_data 12345# 'setosa' = 0# 'versicolor' = 1# 'virginica' = 2kind = train_data['species'].drop_duplicates().to_numpy() # 꽃 종류kind array(['setosa', 'versicolor', 'virginica'], dtype=object) 12345# 꽃 종류 숫자 변환for num, name in enumerate(kind): y_data[y_data == name] = num print(y_data, type(y_data)) [0 1 2 2 0 2 0 1 1 1 1 2 1 0 0 2 2 2 0 1 1 0 1 2 2 2 2 2 2 2 1 0 2 2 2 2 2 0 1 1 1 2 1 0 2 1 1 1 1 1 0 1 0 1 0 1 2 0 2 2 2 2 2 2 0 2 1 1 1 2 1 1 1 1 1] &lt;class 'numpy.ndarray'&gt; 12345# 'setosa' = 0# 'versicolor' = 1# 'virginica' = 2kind_index = 0print(x_data[y_data == kind_index]) [[4.4 1.4 2.9 0.2] [4.9 1.4 3. 0.2] [4.3 1.1 3. 0.1] [4.6 1.5 3.1 0.2] [5.8 1.2 4. 0.2] [5.1 1.4 3.5 0.2] [5.4 1.7 3.9 0.4] [5. 1.5 3.4 0.2] [4.9 1.5 3.1 0.1] [4.7 1.3 3.2 0.2] [4.6 1.4 3.4 0.3] [5. 1.4 3.6 0.2] [5.4 1.5 3.7 0.2] [4.8 1.4 3. 0.1] [4.8 1.6 3.4 0.2]] 12345# 'setosa' = 0# 'versicolor' = 1# 'virginica' = 2kind_index = 1print(x_data[y_data == kind_index]) [[6.4 4.5 3.2 1.5] [6.7 5. 3. 1.7] [6.8 4.8 2.8 1.4] [6.6 4.4 3. 1.4] [5. 3.5 2. 1. ] [6.3 4.7 3.3 1.6] [6.7 4.4 3.1 1.4] [5.5 4. 2.3 1.3] [5.2 3.9 2.7 1.4] [6.4 4.3 2.9 1.3] [5.6 3.9 2.5 1.1] [5.8 4.1 2.7 1. ] [5.9 4.2 3. 1.5] [6. 4.5 2.9 1.5] [6.3 4.9 2.5 1.5] [6.1 4. 2.8 1.3] [6.1 4.7 2.8 1.2] [5.7 4.5 2.8 1.3] [6.6 4.6 2.9 1.3] [6.1 4.7 2.9 1.4] [6. 4. 2.2 1. ] [5.9 4.8 3.2 1.8] [7. 4.7 3.2 1.4] [5.6 4.5 3. 1.5] [5.7 3.5 2.6 1. ] [6.5 4.6 2.8 1.5] [5.6 3.6 2.9 1.3] [6.2 4.5 2.2 1.5] [4.9 3.3 2.4 1. ] [6.9 4.9 3.1 1.5]] 12345# 'setosa' = 0# 'versicolor' = 1# 'virginica' = 2kind_index = 2print(x_data[y_data == kind_index]) [[6.2 4.8 2.8 1.8] [7.2 6.1 3.6 2.5] [6.5 5.8 3. 2.2] [6.3 4.9 2.7 1.8] [7.6 6.6 3. 2.1] [7.7 6.9 2.6 2.3] [7.1 5.9 3. 2.1] [6. 5. 2.2 1.5] [6.8 5.5 3. 2.1] [7.3 6.3 2.9 1.8] [6.9 5.7 3.2 2.3] [4.9 4.5 2.5 1.7] [6.4 5.6 2.8 2.1] [7.7 6.7 3.8 2.2] [6.3 6. 3.3 2.5] [6.7 5.8 2.5 1.8] [6.5 5.1 3.2 2. ] [7.7 6.7 2.8 2. ] [7.2 6. 3.2 1.8] [6.4 5.3 2.7 1.9] [6.3 5.6 2.9 1.8] [6.7 5.7 3.3 2.1] [6.5 5.5 3. 1.8] [6.4 5.3 3.2 2.3] [5.6 4.9 2.8 2. ] [5.8 5.1 2.8 2.4] [7.2 5.8 3. 1.6] [5.8 5.1 2.7 1.9] [6.1 4.9 3. 1.8] [5.7 5. 2.5 2. ]] 2. 데이터 시각화 데이터를 matplotlib.pyplot 라이브러리를 이용항 시각화 합니다. 12import matplotlib.pyplot as pltimport numpy as np 1234567891011121314151617# 'setosa' = 0# 'versicolor' = 1# 'virginica' = 2print(x_data.shape)print(y_data.shape)plt.bar(x_data[y_data == 0, 0], x_data[y_data == 0, 2], width= 0.1, color='r')plt.bar(x_data[y_data == 1, 0], x_data[y_data == 1, 2], width= 0.1, color='g')plt.bar(x_data[y_data == 2, 0], x_data[y_data == 2, 2], width= 0.1, color='b')plt.legend(kind)plt.show()plt.bar(x_data[y_data == 0, 1], x_data[y_data == 0, 3], width= 0.1, color='r')plt.bar(x_data[y_data == 1, 1], x_data[y_data == 1, 3], width= 0.1, color='g')plt.bar(x_data[y_data == 2, 1], x_data[y_data == 2, 3], width= 0.1, color='b')plt.legend(kind)plt.show() (75, 4) (75,) 12plt.plot(y_data, marker='o', linestyle='')plt.show() 3. 데이터 전처리 y 데이터 값을 원-핫 벡터로 코딩합니다. 학습데이터와 테스트데이터를 분리합니다. sklearn.model_selection에서 제공하는 train_test_split 를 사용합니다. One-hot 코딩1234567print(kind)# x_data# y_data# 'setosa' = 0# 'versicolor' = 1# 'virginica' = 2 ['setosa' 'versicolor' 'virginica'] 1234from tensorflow.keras.utils import to_categoricalY_onehot = to_categorical(y_data, num_classes=3)Y_onehot.shape (75, 3) train-test split1from sklearn.model_selection import train_test_split 1234567x_train, x_test, y_train, y_test = train_test_split(x_data, Y_onehot, test_size=0.2, random_state=42)print(x_train.shape)print(y_train.shape)print(x_test.shape)print(y_test.shape) (60, 4) (60, 3) (15, 4) (15, 3) 4. 모델학습 softmax regression을 사용하여 훈련하고, 테스트셋을 이용해 성능을 평가합니다. decision tree를 사용하여 훈련하고, 테스트셋을 이용해 성능을 평가합니다. 이 데이터셋에 적합한 세번째 모델을 찾아보고 훈련해 봅니다. Softmax regression123from tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Densefrom tensorflow.keras import optimizers 12345model = Sequential()model.add(Dense(3, input_dim = 4, activation='softmax'))model.summary() Model: &quot;sequential_6&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_6 (Dense) (None, 3) 15 ================================================================= Total params: 15 Trainable params: 15 Non-trainable params: 0 _________________________________________________________________ 12345W = model.get_weights()[0]b = model.get_weights()[1]print('W', W)print('b', b) W [[-0.34640932 0.649112 -0.27851325] [ 0.44380236 -0.80450916 0.43582928] [ 0.7131357 -0.1360429 -0.01573229] [ 0.630263 0.618158 0.3201201 ]] b [0. 0. 0.] 123model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(learning_rate=0.01), metrics=['acc']) 1history = model.fit(x_train, y_train, epochs=500) 123456789plt.plot(history.history['acc'])plt.title('Accuracy')plt.legend(['train'], loc='upper left')plt.show()plt.plot(history.history['loss'])plt.title('Loss')plt.legend(['loss'], loc='upper left')plt.show() 1score = model.evaluate(x_test, y_test, verbose=1) WARNING:tensorflow:5 out of the last 5 calls to &lt;function Model.make_test_function.&lt;locals&gt;.test_function at 0x7f1c42c01dd0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for more details. 1/1 [==============================] - 0s 114ms/step - loss: 0.3461 - acc: 1.0000 1print(&quot;accuracy :&quot;, score[1]) accuracy : 1.0 1print(&quot;loss :&quot;, score[0]) loss : 0.3460531234741211 Decision Tree1from sklearn import tree 1dt = tree.DecisionTreeClassifier() 1dt.fit(x_train, y_train) DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort='deprecated', random_state=None, splitter='best') 1dt.score(x_test, y_test) 1.0 1tree.plot_tree(dt) [Text(152.1818181818182, 199.32, 'X[3] &lt;= 1.55\\ngini = 0.422\\nsamples = 60\\nvalue = [[49, 11]\\n[35, 25]\\n[36, 24]]'), Text(60.872727272727275, 163.07999999999998, 'X[1] &lt;= 2.5\\ngini = 0.312\\nsamples = 35\\nvalue = [[24, 11]\\n[12, 23]\\n[34, 1]]'), Text(30.436363636363637, 126.83999999999999, 'gini = 0.0\\nsamples = 11\\nvalue = [[0, 11]\\n[11, 0]\\n[11, 0]]'), Text(91.30909090909091, 126.83999999999999, 'X[1] &lt;= 4.95\\ngini = 0.053\\nsamples = 24\\nvalue = [[24, 0]\\n[1, 23]\\n[23, 1]]'), Text(60.872727272727275, 90.6, 'gini = 0.0\\nsamples = 23\\nvalue = [[23, 0]\\n[0, 23]\\n[23, 0]]'), Text(121.74545454545455, 90.6, 'gini = 0.0\\nsamples = 1\\nvalue = [[1, 0]\\n[1, 0]\\n[0, 1]]'), Text(243.4909090909091, 163.07999999999998, 'X[1] &lt;= 5.15\\ngini = 0.098\\nsamples = 25\\nvalue = [[25, 0]\\n[23, 2]\\n[2, 23]]'), Text(213.05454545454546, 126.83999999999999, 'X[2] &lt;= 2.9\\ngini = 0.272\\nsamples = 7\\nvalue = [[7, 0]\\n[5, 2]\\n[2, 5]]'), Text(182.61818181818182, 90.6, 'gini = 0.0\\nsamples = 4\\nvalue = [[4, 0]\\n[4, 0]\\n[0, 4]]'), Text(243.4909090909091, 90.6, 'X[3] &lt;= 1.75\\ngini = 0.296\\nsamples = 3\\nvalue = [[3, 0]\\n[1, 2]\\n[2, 1]]'), Text(213.05454545454546, 54.359999999999985, 'gini = 0.0\\nsamples = 1\\nvalue = [[1, 0]\\n[0, 1]\\n[1, 0]]'), Text(273.92727272727274, 54.359999999999985, 'X[0] &lt;= 6.0\\ngini = 0.333\\nsamples = 2\\nvalue = [[2, 0]\\n[1, 1]\\n[1, 1]]'), Text(243.4909090909091, 18.119999999999976, 'gini = 0.0\\nsamples = 1\\nvalue = [[1, 0]\\n[0, 1]\\n[1, 0]]'), Text(304.3636363636364, 18.119999999999976, 'gini = 0.0\\nsamples = 1\\nvalue = [[1, 0]\\n[1, 0]\\n[0, 1]]'), Text(273.92727272727274, 126.83999999999999, 'gini = 0.0\\nsamples = 18\\nvalue = [[18, 0]\\n[18, 0]\\n[0, 18]]')] 1 내가찾은모델1 5. 모델 비교 어떤 모델이 가장 좋은지 간략히 브리핑 합니다. 1","link":"/2021/10/27/DeepLearning/Iris_dataset/%EA%B3%BC%EC%A0%9C1_Iris_dataset(%EC%A2%85%EB%A5%98%20%EC%98%88%EC%B8%A1)/"},{"title":"딥러닝 공부를 해보자","text":"로지스틱 회귀 ( Logistic Regression ) 목차 개념정리 손실함수 : 크로스 엔트로피 (Cross Entropy) 실습 12345import numpy as npimport matplotlib.pyplot as pltfrom tensorflow.keras.models import Sequentialfrom tensorflow.keras.layers import Densefrom tensorflow.keras import optimizers 1. 개념정리 ### &lt; 가설 &gt; $f(x)=\\sigma(xW+b)$ &lt; 목적 &gt; $(w^{}, b^{})=arg ; min ;J(y, \\hat{y})$ $ =arg ; min ;J(y, f(x))$ $ =arg ; min ;J(y, \\sigma(xW+b))$시그모이드 함수 ( Sigmoid funtion ) $H(x) = \\frac{1}{1+e^{(-x)}} = \\sigma(x)$ e(e=2.718281..): 자연 상수 구현 12def sigmoid(x): return 1/(1+np.exp(-x)) 시각화 12345678910x_arr = np.arange(-5, 5 , 0.1)y_arr = []for x in x_arr: y = sigmoid(x) y_arr.append(y)y_arr = np.array(y_arr)plt.plot([0,0],[0,1],linestyle='--')plt.plot(x_arr, y_arr) [&lt;matplotlib.lines.Line2D at 0x7fd45be78b10&gt;] 가설 구현###$f(x)=\\sigma(xW+b)$ 12def hypothesis(x, W, b=0): return sigmoid(np.dot(x,W)+b) 1 예제 라운드점수(X) 종합점수 결과(Y) -3 1 패배 -2 1 패배 -1 2 패배 0 3 패배 1 5 승리 2 8 승리 3 9 승리 데이터 정의 123456789X = np.arange(-3,4,1).reshape(-1, 1)Y = np.array([[0], [0], [0], [0], [1], [1], [1]])X[0].shape, Y[0].shape ((1,), (1,)) 예측 123456W = np.array([[1]])W.shapefor x in X: y_pred = hypothesis(x[0], W) print(y_pred) [[0.04742587]] [[0.11920292]] [[0.26894142]] [[0.5]] [[0.73105858]] [[0.88079708]] [[0.95257413]] 시각화 12plt.plot(X, Y, marker='o', linestyle='', color='k')plt.plot(X, [hypothesis(x,W) for x in X], color='g') [&lt;matplotlib.lines.Line2D at 0x7fd45c03a4d0&gt;] 학습 파라미터 W 값에 따른 함수의 변화 12345678910plt.plot(X, Y, marker='o', linestyle='', color='k')W = np.array([[0.5]])plt.plot(X, [hypothesis(x,W) for x in X], color='r')W = np.array([[1]])plt.plot(X, [hypothesis(x,W) for x in X], color='g')W = np.array([[2]])plt.plot(X, [hypothesis(x,W) for x in X], color='b')plt.legend(['data','w=0.5','w=1.0','w=2.0'])plt.show() b값에 따른 함수의 변화 123456789101112plt.plot(X, Y, marker='o', linestyle='', color='k')W = np.array([[2]])b = -1plt.plot(X, [hypothesis(x,W,b) for x in X], color='r', marker='.')b = 0plt.plot(X, [hypothesis(x,W,b) for x in X], color='g', marker='.')b = 1plt.plot(X, [hypothesis(x,W,b) for x in X], color='b', marker='.')plt.legend(['data','b=-1','b=0','b=1'])plt.show() 2. 손실함수: 크로스 엔트로피 (Cross Entropy) $J(W) = -\\frac{1}{n}\\sum_{i=1}^{n}[y^{(i)}log(\\hat{y}^{(i)})+(1-y^{(i)})log(1-\\hat{y}^{(i)})]$ $y = 1 \\rightarrow cost(y, \\hat{y}) = -log(\\hat{y})$ $y = 0 \\rightarrow cost(y, \\hat{y}) = -log(1-\\hat{y})$ 구현 12345678910def J(y, y_pred): if y == 1: return -(np.log10(y_pred)) elif y == 0: return -(np.log10(1-y_pred)) else: return 0 시각화 123456789y_pred_arr = np.arange(0.1, 1, 0.1)for y_pred in y_pred_arr: plt.plot(y_pred, J(1, y_pred), marker='o', color='b') plt.plot(y_pred, J(0, y_pred), marker='o', color='r')plt.legend(['y = 1', 'y = 0'])plt.xlabel('y_pred')plt.ylabel('Loss = J(y, y_pred)')plt.show() 3. 실습 데이터 공부시간 집중도 수면시간 종합성적 합격여부 0 1 9 0 불합격 1 1 8.5 1.1 불합격 2 2 8 2.3 불합격 3 4 8 3.0 불합격 4 3 7 4.4 불합격 5 5 7.5 5.5 합격 6 6 7 6.1 합격 7 6 6 7.3 합격 8 7 7 8.4 합격 9 6 6.5 9.8 합격 데이터 정의 1234567891011X = np.array([[0, 1, 9, 0], [1, 1, 8.5, 1.1], [2, 2, 8, 2.3], [3, 4, 8, 3.0], [4, 3, 7, 4.4], [5, 5, 7.5, 5.5], [6, 6, 7, 6.1], [7, 6, 6, 7.3], [8, 7, 7, 8.4], [9, 6, 6.5, 9.8]])Y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1]) 시각화 1plt.plot(Y, linestyle='--', marker='o') [&lt;matplotlib.lines.Line2D at 0x7fd46055cad0&gt;] 모델 생성 케라스를 이용한 모델 구현 123model = Sequential()model.add(Dense(1, input_dim=4, activation='sigmoid'))model.summary() Model: &quot;sequential_9&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_8 (Dense) (None, 1) 5 ================================================================= Total params: 5 Trainable params: 5 Non-trainable params: 0 _________________________________________________________________ 학습 파라미터 확인 1234W = model.get_weights()[0]b = model.get_weights()[1]print('W:',W,'b:',b) W: [[ 0.6756476 ] [-0.5556424 ] [-0.47744548] [ 0.00187528]] b: [0.] 시각화 123456Y_pred = model.predict(X)print(Y.shape, Y_pred.shape)plt.plot(Y, linestyle='--', marker='o')plt.plot(Y_pred, linestyle='--', marker='.')plt.show() WARNING:tensorflow:5 out of the last 9 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x7fd4582275f0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for more details. (10,) (10, 1) 모델 학습 모델 컴파일 1234model.compile(optimizer=optimizers.SGD(learning_rate=0.01), loss='binary_crossentropy', metrics=['acc'] ) 학습 진행 1model.fit(X, Y, epochs=1) 1/1 [==============================] - 0s 4ms/step - loss: 0.1359 - acc: 1.0000 &lt;keras.callbacks.History at 0x7fd460462450&gt; 결과 학습 파라미터 확인 1234W = model.get_weights()[0]b = model.get_weights()[1]print('W:',W,'b:',b) W: [[ 1.0008601 ] [-0.14521359] [-0.7228296 ] [ 0.33016428]] b: [-0.03003764] 시각화 123456Y_pred = model.predict(X)print(Y.shape, Y_pred.shape)plt.plot(Y, linestyle='--', marker='o')plt.plot(Y_pred, linestyle='--', marker='.')plt.show() (10,) (10, 1)","link":"/2021/10/27/DeepLearning/LogisticRegression/2_LogisticRegression/"}],"tags":[{"name":"MLP","slug":"MLP","link":"/tags/MLP/"},{"name":"MNIST","slug":"MNIST","link":"/tags/MNIST/"},{"name":"GitHub","slug":"GitHub","link":"/tags/GitHub/"},{"name":"GitBlog","slug":"GitBlog","link":"/tags/GitBlog/"},{"name":"Threading","slug":"Threading","link":"/tags/Threading/"},{"name":"DeepLearning","slug":"DeepLearning","link":"/tags/DeepLearning/"},{"name":"Cifar10","slug":"Cifar10","link":"/tags/Cifar10/"},{"name":"Softmax Regression","slug":"Softmax-Regression","link":"/tags/Softmax-Regression/"},{"name":"DNN","slug":"DNN","link":"/tags/DNN/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"Logistic Regression","slug":"Logistic-Regression","link":"/tags/Logistic-Regression/"},{"name":"Fashion_Minist","slug":"Fashion-Minist","link":"/tags/Fashion-Minist/"},{"name":"Iris","slug":"Iris","link":"/tags/Iris/"}],"categories":[{"name":"MLP","slug":"MLP","link":"/categories/MLP/"},{"name":"GitHub","slug":"GitHub","link":"/categories/GitHub/"},{"name":"GitBlog","slug":"GitHub/GitBlog","link":"/categories/GitHub/GitBlog/"},{"name":"Threading","slug":"Threading","link":"/categories/Threading/"},{"name":"DeepLearning","slug":"DeepLearning","link":"/categories/DeepLearning/"}]}