{"pages":[],"posts":[{"title":"GitHub를 사용하기위한 기본 세팅","text":"사용환경 Window 10 Git 홈페이지 Download 클릭 사용중인 OS 클릭 다운로드 경로 이동하여 실행파일(.exe) 실행 Git 2.33.1 Setup 실행후 Next 클릭 Install 클릭 Finish 클릭 Git 설치완료","link":"/2021/10/22/GitHub/GitHub-setting-1/"},{"title":"MLP 모델을 활용하여 MNIST 모델 개발","text":"개발환경 Window 10 Python 사용 라이브러리 numpy matplotlib torch 사용 데이터 MNIST 사용 개발 모델 MLP(Multi Layer Perceptron) MLP 모델이란? 퍼셉트론이 지니고 있는 한계(비선형 분류 문제 해결)를 극복하기 위해 여러 Layer를 쌓아올린 MLP(Multi Layer Perceptron)가 등장함. 간단히 비교하면, 퍼셉트론은 Input Layer와 Output Layer만 존재하는 형태입니다. MLP는 Input과 Output 사이에 Hidden Layer를 추가합니다. 즉, MLP는 이러한 Hidden Layer를 여러겹으로 쌓게 되는데, 이를 MLP 모델이라고 부릅니다. MLP 모델을 활용한 MNIST 모델 개발 MLP 모델 순서대로 코드를 구현합니다. Step 1. 모듈 불러오기123456import numpy as npimport matplotlib.pyplot as pltimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torchvision import transforms, datasets Step 2. 딥러닝 모델 설계 시 필요한 장비 세팅123456if torch.cuda.is_available(): DEVICE = torch.device('cuda')else: DEVICE = torch.device('cpu')print(&quot;PyTorch Version:&quot;, torch.__version__, ' Device:', DEVICE) PyTorch Version: 1.9.0+cu111 Device: cuda 12BATCH_SIZE = 32 # 데이터가 32개로 구성되어 있음. EPOCHS = 10 # 전체 데이터 셋을 10번 반복해 학습함. Step 3. 데이터 다운로드 torchvision 내 datasets 함수 이용하여 데이터셋 다운로드 합니다. ToTensor() 활용하여 데이터셋을 tensor 형태로 변환 한 픽셀은 0255 범위의 스칼라 값으로 구성, 이를 01 범위에서 정규화 과정 진행 DataLoader는 일종의 Batch Size 만큼 묶음으로 묶어준다는 의미 Batch_size는 Mini-batch 1개 단위를 구성하는 데이터의 개수 12345678910111213141516train_dataset = datasets.MNIST(root = &quot;../data/MNIST&quot;, train = True, download = True, transform = transforms.ToTensor())test_dataset = datasets.MNIST(root = &quot;../data/MNIST&quot;, train = False, transform = transforms.ToTensor())train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = BATCH_SIZE, shuffle=True)test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = BATCH_SIZE, shuffle=False) step 4. 데이터 확인 및 시각화 데이터를 확인하고 시각화를 진행합니다. 32개의 이미지 데이터에 label 값이 각 1개씩 존재하기 때문에 32개의 값을 갖고 있음 1234for (X_train, y_train) in train_loader: print('X_train:', X_train.size(), 'type:', X_train.type()) print('y_train:', y_train.size(), 'type:', y_train.type()) break X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor y_train: torch.Size([32]) type: torch.LongTensor 1234567pltsize = 1plt.figure(figsize=(10 * pltsize, pltsize))for i in range(10): plt.subplot(1, 10, i + 1) plt.axis('off') plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap=&quot;gray_r&quot;) plt.title('Class: ' + str(y_train[i].item())) step 5. MLP 모델 설계 torch 모듈을 이용해 MLP를 설계합니다. 12345678910111213141516171819class Net(nn.Module): ''' Forward Propagation 정의 ''' def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(28 * 28 * 1, 512) # (가로 픽셀 * 세로 픽셀 * 채널 수) 크기의 노드 수 설정 Fully Connected Layer 노드 수 512개 설정 self.fc2 = nn.Linear(512, 256) # Input으로 사용할 노드 수는 512으로, Output 노드수는 256개로 지정 self.fc3 = nn.Linear(256, 10) # Input 노드수는 256, Output 노드수는 10개로 지정 def forward(self, x): x = x.view(-1, 28 * 28) # 1차원으로 펼친 이미지 데이터 통과 x = self.fc1(x) x = F.sigmoid(x) x = self.fc2(x) x = F.sigmoid(x) x = self.fc3(x) x = F.log_softmax(x, dim = 1) return x step 6. 옵티마이저 목적 함수 설정 Back Propagation 설정 위한 목적 함수 설정 1234model = Net().to(DEVICE)optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.5)criterion = nn.CrossEntropyLoss() # output 값과 원-핫 인코딩 값과의 Loss print(model) Net( (fc1): Linear(in_features=784, out_features=512, bias=True) (fc2): Linear(in_features=512, out_features=256, bias=True) (fc3): Linear(in_features=256, out_features=10, bias=True) ) step 7. MLP 모델 학습 MLP 모델을 학습 상태로 지정하는 코드를 구현 1234567891011121314def train(model, train_loader, optimizer, log_interval): model.train() for batch_idx, (image, label) in enumerate(train_loader): # 모형 학습 image = image.to(DEVICE) label = label.to(DEVICE) optimizer.zero_grad() # Optimizer의 Gradient 초기화 output = model(image) loss = criterion(output, label) loss.backward() # back propagation 계산 optimizer.step() if batch_idx % log_interval == 0: print(&quot;Train Epoch: {} [{}/{}({:.0f}%)]\\tTrain Loass: {:.6f}&quot;.format(Epoch, batch_idx * len(image), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) step 8. 검증 데이터 확인 함수1234567891011121314151617def evaluate(model, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for image, label in test_loader: image = image.to(DEVICE) label = label.to(DEVICE) output = model(image) test_loss += criterion(output, label).item() prediction = output.max(1, keepdim = True)[1] correct += prediction.eq(label.view_as(prediction)).sum().item() test_loss /= len(test_loader.dataset) test_accuracy = 100. * correct / len(test_loader.dataset) return test_loss, test_accuracy 모델 평가 시, Gradient를 통해 파라미터 값이 업데이트되는 현상 방지 위해 torch.no_grad() Gradient의 흐름 제어 step 9. MLP 학습 실행1234for Epoch in range(1, EPOCHS + 1): train(model, train_loader, optimizer, log_interval=200) test_loss, test_accuracy = evaluate(model, test_loader) print(&quot;\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} %\\n&quot;.format(Epoch, test_loss, test_accuracy)) /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead. warnings.warn(&quot;nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.&quot;) Train Epoch: 1 [0/60000(0%)] Train Loass: 2.271369 Train Epoch: 1 [6400/60000(11%)] Train Loass: 2.338554 Train Epoch: 1 [12800/60000(21%)] Train Loass: 2.303339 Train Epoch: 1 [19200/60000(32%)] Train Loass: 2.286205 Train Epoch: 1 [25600/60000(43%)] Train Loass: 2.301424 Train Epoch: 1 [32000/60000(53%)] Train Loass: 2.316196 Train Epoch: 1 [38400/60000(64%)] Train Loass: 2.281273 Train Epoch: 1 [44800/60000(75%)] Train Loass: 2.274917 Train Epoch: 1 [51200/60000(85%)] Train Loass: 2.223652 Train Epoch: 1 [57600/60000(96%)] Train Loass: 2.307122 [EPOCH: 1], Test Loss: 0.0703, Test Accuracy: 27.32 % Train Epoch: 2 [0/60000(0%)] Train Loass: 2.231371 Train Epoch: 2 [6400/60000(11%)] Train Loass: 2.193915 Train Epoch: 2 [12800/60000(21%)] Train Loass: 2.192727 Train Epoch: 2 [19200/60000(32%)] Train Loass: 2.111516 Train Epoch: 2 [25600/60000(43%)] Train Loass: 1.988003 Train Epoch: 2 [32000/60000(53%)] Train Loass: 1.900740 Train Epoch: 2 [38400/60000(64%)] Train Loass: 1.676852 Train Epoch: 2 [44800/60000(75%)] Train Loass: 1.568495 Train Epoch: 2 [51200/60000(85%)] Train Loass: 1.486937 Train Epoch: 2 [57600/60000(96%)] Train Loass: 1.287047 [EPOCH: 2], Test Loss: 0.0397, Test Accuracy: 63.09 % Train Epoch: 3 [0/60000(0%)] Train Loass: 1.430903 Train Epoch: 3 [6400/60000(11%)] Train Loass: 1.077034 Train Epoch: 3 [12800/60000(21%)] Train Loass: 1.064919 Train Epoch: 3 [19200/60000(32%)] Train Loass: 0.905477 Train Epoch: 3 [25600/60000(43%)] Train Loass: 0.938080 Train Epoch: 3 [32000/60000(53%)] Train Loass: 0.872440 Train Epoch: 3 [38400/60000(64%)] Train Loass: 0.839272 Train Epoch: 3 [44800/60000(75%)] Train Loass: 0.676609 Train Epoch: 3 [51200/60000(85%)] Train Loass: 0.697175 Train Epoch: 3 [57600/60000(96%)] Train Loass: 0.629446 [EPOCH: 3], Test Loss: 0.0239, Test Accuracy: 76.73 % Train Epoch: 4 [0/60000(0%)] Train Loass: 0.853515 Train Epoch: 4 [6400/60000(11%)] Train Loass: 0.748754 Train Epoch: 4 [12800/60000(21%)] Train Loass: 0.794022 Train Epoch: 4 [19200/60000(32%)] Train Loass: 0.503913 Train Epoch: 4 [25600/60000(43%)] Train Loass: 0.641133 Train Epoch: 4 [32000/60000(53%)] Train Loass: 0.721150 Train Epoch: 4 [38400/60000(64%)] Train Loass: 0.780429 Train Epoch: 4 [44800/60000(75%)] Train Loass: 0.578635 Train Epoch: 4 [51200/60000(85%)] Train Loass: 0.703849 Train Epoch: 4 [57600/60000(96%)] Train Loass: 0.439490 [EPOCH: 4], Test Loss: 0.0175, Test Accuracy: 83.77 % Train Epoch: 5 [0/60000(0%)] Train Loass: 0.805851 Train Epoch: 5 [6400/60000(11%)] Train Loass: 0.530511 Train Epoch: 5 [12800/60000(21%)] Train Loass: 0.668718 Train Epoch: 5 [19200/60000(32%)] Train Loass: 0.360271 Train Epoch: 5 [25600/60000(43%)] Train Loass: 0.665644 Train Epoch: 5 [32000/60000(53%)] Train Loass: 0.437206 Train Epoch: 5 [38400/60000(64%)] Train Loass: 0.590770 Train Epoch: 5 [44800/60000(75%)] Train Loass: 0.462136 Train Epoch: 5 [51200/60000(85%)] Train Loass: 0.324244 Train Epoch: 5 [57600/60000(96%)] Train Loass: 0.340563 [EPOCH: 5], Test Loss: 0.0143, Test Accuracy: 86.79 % Train Epoch: 6 [0/60000(0%)] Train Loass: 0.449360 Train Epoch: 6 [6400/60000(11%)] Train Loass: 0.412948 Train Epoch: 6 [12800/60000(21%)] Train Loass: 0.861725 Train Epoch: 6 [19200/60000(32%)] Train Loass: 0.261632 Train Epoch: 6 [25600/60000(43%)] Train Loass: 0.420589 Train Epoch: 6 [32000/60000(53%)] Train Loass: 0.343440 Train Epoch: 6 [38400/60000(64%)] Train Loass: 0.583203 Train Epoch: 6 [44800/60000(75%)] Train Loass: 0.677286 Train Epoch: 6 [51200/60000(85%)] Train Loass: 0.410981 Train Epoch: 6 [57600/60000(96%)] Train Loass: 0.520130 [EPOCH: 6], Test Loss: 0.0128, Test Accuracy: 88.20 % Train Epoch: 7 [0/60000(0%)] Train Loass: 0.437152 Train Epoch: 7 [6400/60000(11%)] Train Loass: 0.477764 Train Epoch: 7 [12800/60000(21%)] Train Loass: 0.372617 Train Epoch: 7 [19200/60000(32%)] Train Loass: 0.300505 Train Epoch: 7 [25600/60000(43%)] Train Loass: 0.347319 Train Epoch: 7 [32000/60000(53%)] Train Loass: 0.584562 Train Epoch: 7 [38400/60000(64%)] Train Loass: 0.352436 Train Epoch: 7 [44800/60000(75%)] Train Loass: 0.484784 Train Epoch: 7 [51200/60000(85%)] Train Loass: 0.552677 Train Epoch: 7 [57600/60000(96%)] Train Loass: 0.385262 [EPOCH: 7], Test Loss: 0.0118, Test Accuracy: 89.14 % Train Epoch: 8 [0/60000(0%)] Train Loass: 0.217616 Train Epoch: 8 [6400/60000(11%)] Train Loass: 0.445853 Train Epoch: 8 [12800/60000(21%)] Train Loass: 0.299457 Train Epoch: 8 [19200/60000(32%)] Train Loass: 0.296233 Train Epoch: 8 [25600/60000(43%)] Train Loass: 0.375333 Train Epoch: 8 [32000/60000(53%)] Train Loass: 0.142711 Train Epoch: 8 [38400/60000(64%)] Train Loass: 0.271160 Train Epoch: 8 [44800/60000(75%)] Train Loass: 0.379447 Train Epoch: 8 [51200/60000(85%)] Train Loass: 0.343264 Train Epoch: 8 [57600/60000(96%)] Train Loass: 0.303766 [EPOCH: 8], Test Loss: 0.0114, Test Accuracy: 89.65 % Train Epoch: 9 [0/60000(0%)] Train Loass: 0.320234 Train Epoch: 9 [6400/60000(11%)] Train Loass: 0.502022 Train Epoch: 9 [12800/60000(21%)] Train Loass: 0.255956 Train Epoch: 9 [19200/60000(32%)] Train Loass: 0.592139 Train Epoch: 9 [25600/60000(43%)] Train Loass: 0.299304 Train Epoch: 9 [32000/60000(53%)] Train Loass: 0.337499 Train Epoch: 9 [38400/60000(64%)] Train Loass: 0.361548 Train Epoch: 9 [44800/60000(75%)] Train Loass: 0.323064 Train Epoch: 9 [51200/60000(85%)] Train Loass: 0.211570 Train Epoch: 9 [57600/60000(96%)] Train Loass: 0.259323 [EPOCH: 9], Test Loss: 0.0109, Test Accuracy: 89.95 % Train Epoch: 10 [0/60000(0%)] Train Loass: 0.467115 Train Epoch: 10 [6400/60000(11%)] Train Loass: 0.494733 Train Epoch: 10 [12800/60000(21%)] Train Loass: 0.191910 Train Epoch: 10 [19200/60000(32%)] Train Loass: 0.404155 Train Epoch: 10 [25600/60000(43%)] Train Loass: 0.402310 Train Epoch: 10 [32000/60000(53%)] Train Loass: 0.228435 Train Epoch: 10 [38400/60000(64%)] Train Loass: 0.308584 Train Epoch: 10 [44800/60000(75%)] Train Loass: 0.702611 Train Epoch: 10 [51200/60000(85%)] Train Loass: 0.492218 Train Epoch: 10 [57600/60000(96%)] Train Loass: 0.420322 [EPOCH: 10], Test Loss: 0.0105, Test Accuracy: 90.36 % train 함수 실행하면, model은 기존에 정의한 MLP 모델, train_loader는 학습 데이터, optimizer는 SGD, log_interval은 학습이 진행되면서 mini-batch index를 이용해 과정을 모니터링할 수 있도록 출력함. 학습 완료 시, Test Accuracy는 90% 수준의 정확도를 나타냄. -","link":"/2021/10/21/MLP/MLP_1/"},{"title":"new post","text":"","link":"/2021/10/23/new-post/"}],"tags":[{"name":"MLP","slug":"MLP","link":"/tags/MLP/"},{"name":"MNIST","slug":"MNIST","link":"/tags/MNIST/"},{"name":"GitHub","slug":"GitHub","link":"/tags/GitHub/"}],"categories":[{"name":"MLP","slug":"MLP","link":"/categories/MLP/"},{"name":"GitHub","slug":"GitHub","link":"/categories/GitHub/"}]}